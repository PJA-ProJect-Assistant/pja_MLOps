{
  "os": "Linux-6.5.0-35-generic-x86_64-with-glibc2.35",
  "python": "CPython 3.10.12",
  "startedAt": "2025-05-28T05:40:00.849837Z",
  "args": [
    "--config",
    "full_fine_tuning_config.yaml",
    "--output_dir",
    "./llama-3.1-korean-8b-hf-20-epoch"
  ],
  "program": "/root/pja_MLOps/Fine-tuning/2.Finetuning/./1_train_full_fine_tuning.py",
  "codePath": "Fine-tuning/2.Finetuning/1_train_full_fine_tuning.py",
  "git": {
    "remote": "https://github.com/PJA-ProJect-Assistant/pja_MLOps.git",
    "commit": "4ba293f6a30ddeb7c9477e98214117803e08d4cd"
  },
  "email": "mir960609@gmail.com",
  "root": "/root/pja_MLOps/Fine-tuning/2.Finetuning",
  "host": "5c1baa132ad4",
  "executable": "/usr/bin/python",
  "codePathLocal": "1_train_full_fine_tuning.py",
  "cpu_count": 32,
  "cpu_count_logical": 64,
  "gpu": "NVIDIA GeForce RTX 4090",
  "gpu_count": 1,
  "disk": {
    "/": {
      "total": "161061273600",
      "used": "28524236800"
    }
  },
  "memory": {
    "total": "540651458560"
  },
  "cpu": {
    "count": 32,
    "countLogical": 64
  },
  "gpu_nvidia": [
    {
      "name": "NVIDIA GeForce RTX 4090",
      "memoryTotal": "25757220864",
      "cudaCores": 16384,
      "architecture": "Ada"
    }
  ],
  "cudaVersion": "12.7"
}