"""
3ê°œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: íŒŒì¸íŠœë‹ vs GPT-4o-mini vs GPT-4o
íŒŒì¸íŠœë‹ íš¨ê³¼ë¥¼ ëª…í™•í•˜ê²Œ ì¸¡ì •í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ë¹„êµ
"""

import os
import json
import time
import pandas as pd
from tqdm import tqdm
import openai
from sacrebleu import corpus_bleu
from rouge_score import rouge_scorer
import numpy as np
import ast
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ========================================================================================
# ì „ì—­ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ========================================================================================

# í…ŒìŠ¤íŠ¸í•  3ê°œ ëª¨ë¸ ì •ì˜
FINETUNED_MODEL = "ft:gpt-4o-mini-2024-07-18:test:pja-erd-finetuning-model:BmOgyrDW:ckpt-step-124"
BASELINE_MINI_MODEL = "gpt-4o-mini"  # íŒŒì¸íŠœë‹ ë² ì´ìŠ¤ ëª¨ë¸
BASELINE_4O_MODEL = "gpt-4o"         # í”Œë˜ê·¸ì‹­ ëª¨ë¸

# API í˜¸ì¶œ ì„¤ì •
TEMPERATURE = 0.2
CSV_FILE_PATH = "hehe.csv"

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ í”„ë¡œì íŠ¸ ì„¤ëª…ê³¼ ìš”êµ¬ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ ERD(Entity Relationship Diagram) ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ê° í…Œì´ë¸”ì˜ êµ¬ì¡°, ì»¬ëŸ¼ ì •ë³´, ê´€ê³„ë¥¼ ëª…í™•í•˜ê²Œ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
JSON í˜•íƒœë¡œ êµ¬ì¡°í™”ëœ ERD ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""

# ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ëª¨ë¸
try:
    semantic_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    SEMANTIC_SIMILARITY_AVAILABLE = True
except:
    print("âš ï¸ ì˜ë¯¸ì  ìœ ì‚¬ë„ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ë©”íŠ¸ë¦­ì€ ì œì™¸ë©ë‹ˆë‹¤.")
    SEMANTIC_SIMILARITY_AVAILABLE = False

# ========================================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ========================================================================================

def safe_parse_json_string(json_str):
    """JSON ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    if pd.isna(json_str) or json_str == '':
        return None
    
    try:
        if isinstance(json_str, (dict, list)):
            return json_str
        
        if isinstance(json_str, str):
            try:
                return ast.literal_eval(json_str)
            except:
                return json.loads(json_str)
    except Exception as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return json_str

def format_data_for_prompt(data):
    """ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì í•©í•œ í˜•íƒœë¡œ í¬ë§·íŒ…"""
    if isinstance(data, str):
        return data
    elif isinstance(data, (dict, list)):
        return json.dumps(data, ensure_ascii=False, indent=2)
    else:
        return str(data)

def extract_text_content(data):
    """JSON ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ë¹„êµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    if isinstance(data, str):
        return data.strip()
    elif isinstance(data, dict):
        text_parts = []
        for key, value in data.items():
            if isinstance(value, str):
                text_parts.append(f"{key}: {value}")
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, str):
                        text_parts.append(item)
                    elif isinstance(item, dict):
                        text_parts.append(json.dumps(item, ensure_ascii=False))
        return " ".join(text_parts)
    elif isinstance(data, list):
        text_parts = []
        for item in data:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                text_parts.append(json.dumps(item, ensure_ascii=False))
        return " ".join(text_parts)
    else:
        return str(data)

# ========================================================================================
# í•µì‹¬ í•¨ìˆ˜ ì •ì˜
# ========================================================================================

def openai_chat_completion(model, user_input, project_info):
    """OpenAI Chat Completion APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        formatted_project_info = format_data_for_prompt(project_info)
        formatted_user_input = format_data_for_prompt(user_input)
        
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"""
í”„ë¡œì íŠ¸ ì •ë³´:
{formatted_project_info}

ìš”êµ¬ì‚¬í•­:
{formatted_user_input}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ERD ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
í…Œì´ë¸” êµ¬ì¡°, ì»¬ëŸ¼ ì •ë³´, ê´€ê³„ë¥¼ í¬í•¨í•œ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
"""}
        ]
        
        response = openai.chat.completions.create(
            model=model,
            messages=messages,
            temperature=TEMPERATURE,
            max_tokens=2000
        )
        
        return response.choices[0].message.content.strip()
    
    except Exception as e:
        print(f"API í˜¸ì¶œ ì˜¤ë¥˜ ({model}): {e}")
        return ""

def calculate_individual_metrics(pred_text, ref_text):
    """
    ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        pred_clean = extract_text_content(pred_text).strip()
        ref_clean = extract_text_content(ref_text).strip()
        
        if not pred_clean or not ref_clean:
            return {
                "BLEU": 0.0,
                "ROUGE-L": 0.0,
                "ì˜ë¯¸ì _ìœ ì‚¬ë„": 0.0
            }
        
        # 1. BLEU ì ìˆ˜ (ê°œë³„ ìƒ˜í”Œìš©)
        try:
            bleu_score = corpus_bleu([pred_clean], [[ref_clean]]).score
        except:
            bleu_score = 0.0
        
        # 2. ROUGE-L ì ìˆ˜
        try:
            scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
            rouge_score = scorer.score(ref_clean, pred_clean)['rougeL'].fmeasure * 100
        except:
            rouge_score = 0.0
        
        # 3. ì˜ë¯¸ì  ìœ ì‚¬ë„
        try:
            if SEMANTIC_SIMILARITY_AVAILABLE:
                pred_embedding = semantic_model.encode([pred_clean])
                ref_embedding = semantic_model.encode([ref_clean])
                similarity = cosine_similarity(pred_embedding, ref_embedding)[0][0] * 100
            else:
                similarity = 0.0
        except:
            similarity = 0.0
        
        return {
            "BLEU": round(bleu_score, 2),
            "ROUGE-L": round(rouge_score, 2),
            "ì˜ë¯¸ì _ìœ ì‚¬ë„": round(similarity, 2)
        }
    
    except Exception as e:
        print(f"ê°œë³„ ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return {
            "BLEU": 0.0,
            "ROUGE-L": 0.0,
            "ì˜ë¯¸ì _ìœ ì‚¬ë„": 0.0
        }
    """ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
        
def calculate_semantic_similarity(predictions, references):
    """
    ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    if not SEMANTIC_SIMILARITY_AVAILABLE:
        return 0.0
    
    try:
        # ê¸¸ì´ ë§ì¶”ê¸° (ì•ˆì „ì¥ì¹˜)
        min_length = min(len(predictions), len(references))
        predictions = predictions[:min_length]
        references = references[:min_length]
        
        pred_texts = [extract_text_content(pred) for pred in predictions]
        ref_texts = [extract_text_content(ref) for ref in references]
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        valid_pairs = [(p, r) for p, r in zip(pred_texts, ref_texts) if p.strip() and r.strip()]
        
        if not valid_pairs:
            return 0.0
        
        valid_preds, valid_refs = zip(*valid_pairs)
        
        pred_embeddings = semantic_model.encode(list(valid_preds))
        ref_embeddings = semantic_model.encode(list(valid_refs))
        
        similarities = []
        for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):
            similarity = cosine_similarity([pred_emb], [ref_emb])[0][0]
            similarities.append(similarity)
        
        return np.mean(similarities) * 100
    except Exception as e:
        print(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        print(f"predictions ê¸¸ì´: {len(predictions) if 'predictions' in locals() else 'N/A'}")
        print(f"references ê¸¸ì´: {len(references) if 'references' in locals() else 'N/A'}")
        return 0.0
    """
    ê° ìƒ˜í”Œë³„ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    """
    print("\nğŸ“Š ìƒ˜í”Œë³„ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘...")
    
    # ê¸°ë³¸ ì •ë³´ ì»¬ëŸ¼ë“¤
    detailed_data = []
    
    model_names = {
        FINETUNED_MODEL: "íŒŒì¸íŠœë‹",
        BASELINE_MINI_MODEL: "4o-mini", 
        BASELINE_4O_MODEL: "4o"
    }
    
    for i in range(len(references)):
        row_data = {
            'sample_id': i + 1,
            'user_input_length': len(str(test_df.iloc[i]['user_input'])),
            'reference_length': len(str(references[i])),
        }
        
        # ê° ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚°
        for model_id, model_name in model_names.items():
            if i < len(results[model_id]):
                prediction = results[model_id][i]
                reference = references[i]
                
                # ê°œë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
                metrics = calculate_individual_metrics(prediction, reference)
                
                # ì»¬ëŸ¼ëª…ì— ëª¨ë¸ëª… ì¶”ê°€
                row_data[f'{model_name}_BLEU'] = metrics['BLEU']
                row_data[f'{model_name}_ROUGE-L'] = metrics['ROUGE-L']
                row_data[f'{model_name}_ì˜ë¯¸ì ìœ ì‚¬ë„'] = metrics['ì˜ë¯¸ì _ìœ ì‚¬ë„']
                row_data[f'{model_name}_output_length'] = len(str(prediction))
        
        # ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°
        if 'íŒŒì¸íŠœë‹_BLEU' in row_data and '4o-mini_BLEU' in row_data:
            row_data['íŒŒì¸íŠœë‹_vs_mini_BLEU'] = row_data['íŒŒì¸íŠœë‹_BLEU'] - row_data['4o-mini_BLEU']
            row_data['íŒŒì¸íŠœë‹_vs_mini_ROUGE'] = row_data['íŒŒì¸íŠœë‹_ROUGE-L'] - row_data['4o-mini_ROUGE-L']
            row_data['íŒŒì¸íŠœë‹_vs_mini_ì˜ë¯¸ì '] = row_data['íŒŒì¸íŠœë‹_ì˜ë¯¸ì ìœ ì‚¬ë„'] - row_data['4o-mini_ì˜ë¯¸ì ìœ ì‚¬ë„']
        
        if 'íŒŒì¸íŠœë‹_BLEU' in row_data and '4o_BLEU' in row_data:
            row_data['íŒŒì¸íŠœë‹_vs_4o_BLEU'] = row_data['íŒŒì¸íŠœë‹_BLEU'] - row_data['4o_BLEU']
            row_data['íŒŒì¸íŠœë‹_vs_4o_ROUGE'] = row_data['íŒŒì¸íŠœë‹_ROUGE-L'] - row_data['4o_ROUGE-L']
            row_data['íŒŒì¸íŠœë‹_vs_4o_ì˜ë¯¸ì '] = row_data['íŒŒì¸íŠœë‹_ì˜ë¯¸ì ìœ ì‚¬ë„'] - row_data['4o_ì˜ë¯¸ì ìœ ì‚¬ë„']
        
        detailed_data.append(row_data)
    
    df_detailed = pd.DataFrame(detailed_data)
    return df_detailed

def analyze_dataframe_insights(df_detailed):
    """
    ë°ì´í„°í”„ë ˆì„ì—ì„œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë° ë¶„ì„
    """
    print("\nğŸ” ë°ì´í„°í”„ë ˆì„ ì¸ì‚¬ì´íŠ¸ ë¶„ì„")
    print("=" * 80)
    
    # 1. ê¸°ë³¸ í†µê³„
    print("ğŸ“ˆ ê¸°ë³¸ í†µê³„:")
    print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {len(df_detailed)}")
    print(f"  í‰ê·  ì…ë ¥ ê¸¸ì´: {df_detailed['user_input_length'].mean():.0f}ì")
    print(f"  í‰ê·  ì •ë‹µ ê¸¸ì´: {df_detailed['reference_length'].mean():.0f}ì")
    
    # 2. ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
    print(f"\nğŸ“Š ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥:")
    models = ['íŒŒì¸íŠœë‹', '4o-mini', '4o']
    metrics = ['BLEU', 'ROUGE-L', 'ì˜ë¯¸ì ìœ ì‚¬ë„']
    
    for model in models:
        print(f"\n  ğŸ¤– {model}:")
        for metric in metrics:
            col_name = f'{model}_{metric}'
            if col_name in df_detailed.columns:
                avg_score = df_detailed[col_name].mean()
                std_score = df_detailed[col_name].std()
                print(f"    {metric}: {avg_score:.2f} (Â±{std_score:.2f})")
    
    # 3. íŒŒì¸íŠœë‹ì´ ìš°ìˆ˜í•œ ìƒ˜í”Œ ë¹„ìœ¨
    print(f"\nğŸ† íŒŒì¸íŠœë‹ ìš°ìˆ˜ ìƒ˜í”Œ ë¶„ì„:")
    
    if 'íŒŒì¸íŠœë‹_vs_mini_BLEU' in df_detailed.columns:
        mini_wins = {
            'BLEU': (df_detailed['íŒŒì¸íŠœë‹_vs_mini_BLEU'] > 0).sum(),
            'ROUGE-L': (df_detailed['íŒŒì¸íŠœë‹_vs_mini_ROUGE'] > 0).sum(),
            'ì˜ë¯¸ì ìœ ì‚¬ë„': (df_detailed['íŒŒì¸íŠœë‹_vs_mini_ì˜ë¯¸ì '] > 0).sum()
        }
        
        print(f"  vs GPT-4o-mini:")
        for metric, wins in mini_wins.items():
            win_rate = (wins / len(df_detailed)) * 100
            print(f"    {metric} ìš°ìˆ˜: {wins}/{len(df_detailed)} ({win_rate:.1f}%)")
    
    if 'íŒŒì¸íŠœë‹_vs_4o_BLEU' in df_detailed.columns:
        gpt4o_wins = {
            'BLEU': (df_detailed['íŒŒì¸íŠœë‹_vs_4o_BLEU'] > 0).sum(),
            'ROUGE-L': (df_detailed['íŒŒì¸íŠœë‹_vs_4o_ROUGE'] > 0).sum(),
            'ì˜ë¯¸ì ìœ ì‚¬ë„': (df_detailed['íŒŒì¸íŠœë‹_vs_4o_ì˜ë¯¸ì '] > 0).sum()
        }
        
        print(f"  vs GPT-4o:")
        for metric, wins in gpt4o_wins.items():
            win_rate = (wins / len(df_detailed)) * 100
            print(f"    {metric} ìš°ìˆ˜: {wins}/{len(df_detailed)} ({win_rate:.1f}%)")
    
    # 4. ìµœê³ /ìµœì € ì„±ëŠ¥ ìƒ˜í”Œ
    print(f"\nğŸ¯ ê·¹ê°’ ë¶„ì„:")
    
    if 'íŒŒì¸íŠœë‹_BLEU' in df_detailed.columns:
        # íŒŒì¸íŠœë‹ ëª¨ë¸ ìµœê³  ì„±ëŠ¥
        best_idx = df_detailed['íŒŒì¸íŠœë‹_BLEU'].idxmax()
        worst_idx = df_detailed['íŒŒì¸íŠœë‹_BLEU'].idxmin()
        
        print(f"  íŒŒì¸íŠœë‹ BLEU ìµœê³ : ìƒ˜í”Œ {best_idx+1} ({df_detailed.loc[best_idx, 'íŒŒì¸íŠœë‹_BLEU']:.2f}ì )")
        print(f"  íŒŒì¸íŠœë‹ BLEU ìµœì €: ìƒ˜í”Œ {worst_idx+1} ({df_detailed.loc[worst_idx, 'íŒŒì¸íŠœë‹_BLEU']:.2f}ì )")
    
    # 5. ì„±ëŠ¥ ë¶„í¬ ë¶„ì„
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¶„í¬ ë¶„ì„:")
    if 'íŒŒì¸íŠœë‹_BLEU' in df_detailed.columns:
        bleu_ranges = {
            '90-100': (df_detailed['íŒŒì¸íŠœë‹_BLEU'] >= 90).sum(),
            '80-89': ((df_detailed['íŒŒì¸íŠœë‹_BLEU'] >= 80) & (df_detailed['íŒŒì¸íŠœë‹_BLEU'] < 90)).sum(),
            '70-79': ((df_detailed['íŒŒì¸íŠœë‹_BLEU'] >= 70) & (df_detailed['íŒŒì¸íŠœë‹_BLEU'] < 80)).sum(),
            '60-69': ((df_detailed['íŒŒì¸íŠœë‹_BLEU'] >= 60) & (df_detailed['íŒŒì¸íŠœë‹_BLEU'] < 70)).sum(),
            '50-59': ((df_detailed['íŒŒì¸íŠœë‹_BLEU'] >= 50) & (df_detailed['íŒŒì¸íŠœë‹_BLEU'] < 60)).sum(),
            '0-49': (df_detailed['íŒŒì¸íŠœë‹_BLEU'] < 50).sum()
        }
        
        print(f"  íŒŒì¸íŠœë‹ BLEU ì ìˆ˜ ë¶„í¬:")
        for range_name, count in bleu_ranges.items():
            percentage = (count / len(df_detailed)) * 100
            print(f"    {range_name}ì : {count}ê°œ ({percentage:.1f}%)")
    
    return df_detailed

def calculate_metrics(predictions, references):
    """ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
    pred_texts = [extract_text_content(pred) for pred in predictions]
    ref_texts = [extract_text_content(ref) for ref in references]
    
    valid_pairs = [(p, r) for p, r in zip(pred_texts, ref_texts) if p.strip() and r.strip()]
    
    if not valid_pairs:
        return {
            "BLEU": 0.0,
            "ROUGE-L": 0.0,
            "ì •í™•ì¼ì¹˜ìœ¨(%)": 0.0,
            "ì˜ë¯¸ì _ìœ ì‚¬ë„(%)": 0.0
        }
    
    valid_preds, valid_refs = zip(*valid_pairs)
    
    # BLEU ì ìˆ˜ ê³„ì‚°
    try:
        bleu_score = corpus_bleu(list(valid_preds), [list(valid_refs)]).score
    except:
        bleu_score = 0.0
    
    # ROUGE-L ì ìˆ˜ ê³„ì‚°
    try:
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        rouge_scores = [scorer.score(ref, pred)['rougeL'].fmeasure 
                       for ref, pred in zip(valid_refs, valid_preds)]
        rouge_l_score = np.mean(rouge_scores) * 100
    except:
        rouge_l_score = 0.0
    
    # ì •í™• ì¼ì¹˜ìœ¨ ê³„ì‚°
    exact_matches = sum(1 for p, r in zip(valid_preds, valid_refs) 
                       if p.strip() == r.strip())
    exact_match_rate = (exact_matches / len(valid_pairs)) * 100
    
    # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
    semantic_similarity = calculate_semantic_similarity(predictions, references)
    
    return {
        "BLEU": round(bleu_score, 2),
        "ROUGE-L": round(rouge_l_score, 2),
        "ì •í™•ì¼ì¹˜ìœ¨(%)": round(exact_match_rate, 2),
        "ì˜ë¯¸ì _ìœ ì‚¬ë„(%)": round(semantic_similarity, 2)
    }

def run_three_model_evaluation():
    """3ê°œ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š 3ê°œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: íŒŒì¸íŠœë‹ vs GPT-4o-mini vs GPT-4o")
    print("=" * 80)
    
    # ========================================================================================
    # 1. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
    # ========================================================================================
    try:
        df = pd.read_csv(CSV_FILE_PATH)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ“‹ CSV ì»¬ëŸ¼: {list(df.columns)}")
            
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    print("\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    for col in ['user_input', 'project_info', 'total_requirements', 'ERD_data']:
        if col in df.columns:
            df[col] = df[col].apply(safe_parse_json_string)
    
    # ========================================================================================
    # 2. 3ê°œ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì „ì²´ ë°ì´í„°)
    # ========================================================================================
    
    # ì „ì²´ ë°ì´í„° ì‚¬ìš© (ì‚¬ìš©ì ì„ íƒ ê°€ëŠ¥)
    use_full_data = input("ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
    
    if use_full_data == 'y':
        test_df = df.copy()
        print(f"\nğŸ”„ ì „ì²´ {len(test_df)}ê°œ ìƒ˜í”Œì— ëŒ€í•´ 3ê°œ ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
        print(f"âš ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {len(test_df) * 0.5}ë¶„ (API í˜¸ì¶œ ì§€ì—° í¬í•¨)")
        print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ì•½ ${len(test_df) * 0.03:.2f} (GPT-4o ê¸°ì¤€)")
        
        proceed = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
        if proceed != 'y':
            print("í…ŒìŠ¤íŠ¸ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    else:
        test_samples = min(10, len(df))  # ê¸°ë³¸ê°’ì„ 10ê°œë¡œ ì¦ê°€
        test_df = df.head(test_samples).copy()
        print(f"\nğŸ”„ {test_samples}ê°œ ìƒ˜í”Œì— ëŒ€í•´ 3ê°œ ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
    
    # ì§„í–‰ ìƒí™© ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€
    checkpoint_file = "evaluation_checkpoint.json"
    
    # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            if len(checkpoint_data.get('completed_indices', [])) > 0:
                print(f"ğŸ“‹ ê¸°ì¡´ ì§„í–‰ìƒí™© ë°œê²¬: {len(checkpoint_data['completed_indices'])}ê°œ ìƒ˜í”Œ ì™„ë£Œ")
                resume = input("ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
                
                if resume == 'y':
                    completed_indices = set(checkpoint_data['completed_indices'])
                    results = checkpoint_data.get('results', {
                        FINETUNED_MODEL: [],
                        BASELINE_MINI_MODEL: [],
                        BASELINE_4O_MODEL: []
                    })
                else:
                    completed_indices = set()
                    results = {
                        FINETUNED_MODEL: [],
                        BASELINE_MINI_MODEL: [],
                        BASELINE_4O_MODEL: []
                    }
            else:
                completed_indices = set()
                results = {
                    FINETUNED_MODEL: [],
                    BASELINE_MINI_MODEL: [],
                    BASELINE_4O_MODEL: []
                }
        except:
            completed_indices = set()
            results = {
                FINETUNED_MODEL: [],
                BASELINE_MINI_MODEL: [],
                BASELINE_4O_MODEL: []
            }
    else:
        completed_indices = set()
        results = {
            FINETUNED_MODEL: [],
            BASELINE_MINI_MODEL: [],
            BASELINE_4O_MODEL: []
        }
    
    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="ëª¨ë¸ ì¶”ë¡  ì§„í–‰"):
        # ì´ë¯¸ ì™„ë£Œëœ ìƒ˜í”Œì€ ê±´ë„ˆë›°ê¸°
        if idx in completed_indices:
            continue
            
        user_input = row['user_input']
        project_info = row['project_info']
        
        print(f"\nğŸ“ ìƒ˜í”Œ {idx+1}/{len(test_df)} ì²˜ë¦¬ ì¤‘...")
        
        try:
            # 1. íŒŒì¸íŠœë‹ ëª¨ë¸ ì¶”ë¡ 
            print(f"   ğŸ¯ íŒŒì¸íŠœë‹ ëª¨ë¸ ì¶”ë¡ ...")
            ft_result = openai_chat_completion(FINETUNED_MODEL, user_input, project_info)
            
            # 2. GPT-4o-mini ì›ë³¸ ì¶”ë¡ 
            print(f"   ğŸ¤– GPT-4o-mini ì¶”ë¡ ...")
            mini_result = openai_chat_completion(BASELINE_MINI_MODEL, user_input, project_info)
            
            # 3. GPT-4o ì¶”ë¡ 
            print(f"   ğŸ”¥ GPT-4o ì¶”ë¡ ...")
            gpt4o_result = openai_chat_completion(BASELINE_4O_MODEL, user_input, project_info)
            
            # ê²°ê³¼ ì €ì¥
            results[FINETUNED_MODEL].append(ft_result)
            results[BASELINE_MINI_MODEL].append(mini_result)
            results[BASELINE_4O_MODEL].append(gpt4o_result)
            
            # ì™„ë£Œëœ ì¸ë±ìŠ¤ ì¶”ê°€
            completed_indices.add(idx)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (10ê°œë§ˆë‹¤)
            if len(completed_indices) % 10 == 0:
                checkpoint_data = {
                    'completed_indices': list(completed_indices),
                    'results': results
                }
                with open(checkpoint_file, 'w', encoding='utf-8') as f:
                    json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                print(f"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: {len(completed_indices)}ê°œ ì™„ë£Œ")
            
            # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€
            time.sleep(1.0)
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if len(completed_indices) % 5 == 0:
                remaining = len(test_df) - len(completed_indices)
                eta_minutes = remaining * 0.5
                print(f"   â±ï¸ ì§„í–‰ë¥ : {len(completed_indices)}/{len(test_df)} ({len(completed_indices)/len(test_df)*100:.1f}%) - ì˜ˆìƒ ì”ì—¬ì‹œê°„: {eta_minutes:.1f}ë¶„")
                
        except Exception as e:
            print(f"   âŒ ìƒ˜í”Œ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"   â­ï¸ ë‹¤ìŒ ìƒ˜í”Œë¡œ ê±´ë„ˆëœë‹ˆë‹¤...")
            continue
    
    # ========================================================================================
    # 3. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¹„êµ
    # ========================================================================================
    
    print("\nğŸ“ˆ 3ê°œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    print("=" * 80)
    
    # ê²°ê³¼ ê¸¸ì´ ë§ì¶”ê¸° (ì‹¤íŒ¨í•œ ìƒ˜í”Œ ì œì™¸)
    min_length = min(len(results[model]) for model in results.keys())
    for model in results.keys():
        results[model] = results[model][:min_length]
    
    # í•´ë‹¹í•˜ëŠ” ì •ë‹µ ë°ì´í„°ë„ ë§ì¶°ì„œ ìë¥´ê¸°
    references = test_df['ERD_data'].tolist()[:min_length]
    test_df_matched = test_df.head(min_length).copy()
    
    print(f"ğŸ“Š ì‹¤ì œ ë¶„ì„ ëŒ€ìƒ: {min_length}ê°œ ìƒ˜í”Œ")
    
    # ========================================================================================
    # 3.1 ìƒì„¸ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    # ========================================================================================
    
    df_detailed = create_detailed_dataframe(test_df_matched, results, references)
    
    # ìƒì„¸ ë°ì´í„°í”„ë ˆì„ ì €ì¥
    try:
        df_detailed.to_csv("ìƒ˜í”Œë³„_ìƒì„¸_ì„±ëŠ¥ë¶„ì„.csv", index=False, encoding="utf-8")
        print(f"ğŸ’¾ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ 'ìƒ˜í”Œë³„_ìƒì„¸_ì„±ëŠ¥ë¶„ì„.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì—‘ì…€ íŒŒì¼ë¡œë„ ì €ì¥ (ë” ë³´ê¸° ì¢‹ìŒ)
        df_detailed.to_excel("ìƒ˜í”Œë³„_ìƒì„¸_ì„±ëŠ¥ë¶„ì„.xlsx", index=False)
        print(f"ğŸ“Š Excel íŒŒì¼ë„ 'ìƒ˜í”Œë³„_ìƒì„¸_ì„±ëŠ¥ë¶„ì„.xlsx'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ìƒì„¸ ë¶„ì„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ë°ì´í„°í”„ë ˆì„ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
    df_detailed = analyze_dataframe_insights(df_detailed)
    
    # ========================================================================================
    # 3.2 ì „ì²´ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
    # ========================================================================================
    
    # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if completed_indices:
        checkpoint_data = {
            'completed_indices': list(completed_indices),
            'results': results
        }
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì „ì²´ ì¶”ë¡  ì™„ë£Œ: {len(completed_indices)}/{len(test_df)} ìƒ˜í”Œ")
    
    # ê²°ê³¼ ê¸¸ì´ ë§ì¶”ê¸° (ì‹¤íŒ¨í•œ ìƒ˜í”Œ ì œì™¸)
    min_length = min(len(results[model]) for model in results.keys())
    for model in results.keys():
        results[model] = results[model][:min_length]
    
    # í•´ë‹¹í•˜ëŠ” ì •ë‹µ ë°ì´í„°ë„ ë§ì¶°ì„œ ìë¥´ê¸°
    references = test_df['ERD_data'].tolist()[:min_length]
    
    # ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚° (ì „ì²´ í‰ê· )
    final_results = {}
    model_names = {
        FINETUNED_MODEL: "íŒŒì¸íŠœë‹_ëª¨ë¸",
        BASELINE_MINI_MODEL: "GPT-4o-mini", 
        BASELINE_4O_MODEL: "GPT-4o"
    }
    
    for model_id, predictions in results.items():
        model_name = model_names[model_id]
        
        print(f"\nğŸ” {model_name} ì¶œë ¥ ìƒ˜í”Œ í™•ì¸:")
        for i, pred in enumerate(predictions[:2]):
            print(f"  ì¶œë ¥ {i+1}: {str(pred)[:150]}...")
        
        metrics = calculate_metrics(predictions, references)
        
        print(f"\nğŸ¤– {model_name} ì „ì²´ í‰ê·  ì„±ëŠ¥:")
        for metric_name, value in metrics.items():
            print(f"   {metric_name}: {value}")
        
        final_results[model_name] = metrics
    
    # ========================================================================================
    # 4. ì¢…í•© ë¹„êµ ë¶„ì„
    # ========================================================================================
    
    print(f"\nğŸ“Š ì¢…í•© ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
    print("=" * 80)
    
    # ì„±ëŠ¥ ë¹„êµ í‘œ ì¶œë ¥
    metrics_list = ["BLEU", "ROUGE-L", "ì˜ë¯¸ì _ìœ ì‚¬ë„(%)"]
    
    print(f"{'ë©”íŠ¸ë¦­':<15} {'íŒŒì¸íŠœë‹':<12} {'4o-mini':<12} {'GPT-4o':<12} {'vs mini':<12} {'vs 4o':<12}")
    print("-" * 80)
    
    for metric in metrics_list:
        ft_score = final_results["íŒŒì¸íŠœë‹_ëª¨ë¸"][metric]
        mini_score = final_results["GPT-4o-mini"][metric]
        gpt4o_score = final_results["GPT-4o"][metric]
        
        vs_mini = ft_score - mini_score
        vs_4o = ft_score - gpt4o_score
        
        print(f"{metric:<15} {ft_score:<12.2f} {mini_score:<12.2f} {gpt4o_score:<12.2f} {vs_mini:<+12.2f} {vs_4o:<+12.2f}")
    
    # ========================================================================================
    # 5. íŒŒì¸íŠœë‹ íš¨ê³¼ ë¶„ì„
    # ========================================================================================
    
    print(f"\nğŸ¯ íŒŒì¸íŠœë‹ íš¨ê³¼ ë¶„ì„")
    print("=" * 80)
    
    ft_metrics = final_results["íŒŒì¸íŠœë‹_ëª¨ë¸"]
    mini_metrics = final_results["GPT-4o-mini"]
    gpt4o_metrics = final_results["GPT-4o"]
    
    # ë² ì´ìŠ¤ ëª¨ë¸(4o-mini) ëŒ€ë¹„ ê°œì„ ë„
    print("ğŸ“ˆ ë² ì´ìŠ¤ ëª¨ë¸(GPT-4o-mini) ëŒ€ë¹„ íŒŒì¸íŠœë‹ ê°œì„ ë„:")
    mini_improvements = 0
    for metric in metrics_list:
        diff = ft_metrics[metric] - mini_metrics[metric]
        improvement_rate = (diff / mini_metrics[metric]) * 100 if mini_metrics[metric] != 0 else 0
        print(f"  {metric}: {diff:+.2f} ({improvement_rate:+.1f}%)")
        if diff > 0:
            mini_improvements += 1
    
    # í”Œë˜ê·¸ì‹­ ëª¨ë¸(GPT-4o) ëŒ€ë¹„ ì„±ëŠ¥
    print(f"\nğŸ”¥ í”Œë˜ê·¸ì‹­ ëª¨ë¸(GPT-4o) ëŒ€ë¹„ íŒŒì¸íŠœë‹ ì„±ëŠ¥:")
    gpt4o_wins = 0
    for metric in metrics_list:
        diff = ft_metrics[metric] - gpt4o_metrics[metric]
        print(f"  {metric}: {diff:+.2f} ({'ìš°ìˆ˜' if diff > 0 else 'ì—´ì„¸'})")
        if diff > 0:
            gpt4o_wins += 1
    
    # ========================================================================================
    # 6. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
    # ========================================================================================
    
    print(f"\nğŸ† ìµœì¢… ê²°ë¡ ")
    print("=" * 80)
    
    if mini_improvements >= 2:
        print("âœ… íŒŒì¸íŠœë‹ì´ ë² ì´ìŠ¤ ëª¨ë¸ ëŒ€ë¹„ ëª…í™•í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤!")
    else:
        print("âš ï¸ íŒŒì¸íŠœë‹ íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤.")
    
    if gpt4o_wins >= 2:
        print("ğŸ”¥ íŒŒì¸íŠœë‹ ëª¨ë¸ì´ í”Œë˜ê·¸ì‹­ ëª¨ë¸ë„ ëŠ¥ê°€í•˜ëŠ” ë†€ë¼ìš´ ì„±ê³¼ì…ë‹ˆë‹¤!")
    elif gpt4o_wins >= 1:
        print("ğŸ‘ íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ì¼ë¶€ ì§€í‘œì—ì„œ í”Œë˜ê·¸ì‹­ ëª¨ë¸ê³¼ ê²½ìŸí•©ë‹ˆë‹¤!")
    else:
        print("ğŸ“ í”Œë˜ê·¸ì‹­ ëª¨ë¸ ëŒ€ë¹„ë¡œëŠ” ì•„ì§ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
    
    # ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„
    print(f"\nğŸ’° ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„:")
    samples_processed = len(completed_indices)
    print(f"  ì²˜ë¦¬ëœ ìƒ˜í”Œ: {samples_processed}ê°œ")
    print(f"  íŒŒì¸íŠœë‹ ëª¨ë¸ ë¹„ìš©: GPT-4oì˜ ~10% (ëŒ€í­ ì ˆì•½)")
    print(f"  ì¶”ë¡  ì†ë„: GPT-4oë³´ë‹¤ ë¹ ë¦„")
    if gpt4o_wins >= 1:
        print(f"  ì„±ëŠ¥: ì¼ë¶€ ì§€í‘œì—ì„œ GPT-4o ìˆ˜ì¤€ ë˜ëŠ” ê·¸ ì´ìƒ")
        print(f"  â†’ ğŸ¯ ë§¤ìš° ë†’ì€ ROI!")
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì •ë¦¬
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        print(f"\nğŸ§¹ ì„ì‹œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ========================================================================================
    # 7. ê²°ê³¼ ì €ì¥ (ìƒì„¸ ë¶„ì„ í¬í•¨)
    # ========================================================================================
    
    try:
        # ì „ì²´ í‰ê·  ê²°ê³¼ ì €ì¥
        with open("3ëª¨ë¸_ì„±ëŠ¥ë¹„êµ_ê²°ê³¼.json", "w", encoding="utf-8") as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ì „ì²´ í‰ê·  ê²°ê³¼ê°€ '3ëª¨ë¸_ì„±ëŠ¥ë¹„êµ_ê²°ê³¼.json' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìƒì„¸ ì¶œë ¥ ê²°ê³¼ ì €ì¥
        output_df = test_df_matched.copy()
        output_df['íŒŒì¸íŠœë‹_ëª¨ë¸_ì¶œë ¥'] = results[FINETUNED_MODEL]
        output_df['GPT4o_mini_ì¶œë ¥'] = results[BASELINE_MINI_MODEL]
        output_df['GPT4o_ì¶œë ¥'] = results[BASELINE_4O_MODEL]
        output_df.to_csv("3ëª¨ë¸_ì „ì²´_ì¶œë ¥_ë¹„êµ.csv", index=False, encoding="utf-8")
        print(f"ğŸ“„ ì „ì²´ ì¶œë ¥ ë¹„êµê°€ '3ëª¨ë¸_ì „ì²´_ì¶œë ¥_ë¹„êµ.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        summary_report = {
            "ì‹¤í—˜_ì •ë³´": {
                "ì´_ìƒ˜í”Œìˆ˜": len(completed_indices),
                "ì„±ê³µ_ìƒ˜í”Œìˆ˜": min_length,
                "í…ŒìŠ¤íŠ¸_ë‚ ì§œ": time.strftime("%Y-%m-%d %H:%M:%S"),
                "ëª¨ë¸_ì •ë³´": {
                    "íŒŒì¸íŠœë‹": FINETUNED_MODEL,
                    "4o-mini": BASELINE_MINI_MODEL,
                    "4o": BASELINE_4O_MODEL
                }
            },
            "ì „ì²´_í‰ê· _ì„±ëŠ¥": final_results,
            "íŒŒì¸íŠœë‹_ê°œì„ ë„": {
                "vs_4o_mini": {
                    metric: final_results["íŒŒì¸íŠœë‹_ëª¨ë¸"][metric] - final_results["GPT-4o-mini"][metric]
                    for metric in ["BLEU", "ROUGE-L", "ì˜ë¯¸ì _ìœ ì‚¬ë„(%)"]
                },
                "vs_4o": {
                    metric: final_results["íŒŒì¸íŠœë‹_ëª¨ë¸"][metric] - final_results["GPT-4o"][metric]
                    for metric in ["BLEU", "ROUGE-L", "ì˜ë¯¸ì _ìœ ì‚¬ë„(%)"]
                }
            }
        }
        
        with open("ì‹¤í—˜_ìš”ì•½_ë¦¬í¬íŠ¸.json", "w", encoding="utf-8") as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ ì‹¤í—˜ ìš”ì•½ ë¦¬í¬íŠ¸ê°€ 'ì‹¤í—˜_ìš”ì•½_ë¦¬í¬íŠ¸.json'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ========================================================================================
    # 8. ìµœì¢… ìš”ì•½ ì¶œë ¥
    # ========================================================================================
    
    print(f"\nğŸŠ ìµœì¢… ì‹¤í—˜ ìš”ì•½")
    print("=" * 80)
    print(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ: {min_length}ê°œ ìƒ˜í”Œ")
    print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"   â€¢ ìƒ˜í”Œë³„_ìƒì„¸_ì„±ëŠ¥ë¶„ì„.csv/.xlsx - ê° ìƒ˜í”Œë³„ ê°œë³„ ì ìˆ˜")
    print(f"   â€¢ 3ëª¨ë¸_ì„±ëŠ¥ë¹„êµ_ê²°ê³¼.json - ì „ì²´ í‰ê·  ì„±ëŠ¥")
    print(f"   â€¢ 3ëª¨ë¸_ì „ì²´_ì¶œë ¥_ë¹„êµ.csv - ëª¨ë“  ëª¨ë¸ ì¶œë ¥ ë¹„êµ")
    print(f"   â€¢ ì‹¤í—˜_ìš”ì•½_ë¦¬í¬íŠ¸.json - ì¢…í•© ì‹¤í—˜ ë¦¬í¬íŠ¸")
    
    # í•µì‹¬ ì„±ê³¼ ìš”ì•½
    ft_bleu = final_results["íŒŒì¸íŠœë‹_ëª¨ë¸"]["BLEU"]
    mini_bleu = final_results["GPT-4o-mini"]["BLEU"]
    gpt4o_bleu = final_results["GPT-4o"]["BLEU"]
    
    print(f"\nğŸ† í•µì‹¬ ì„±ê³¼:")
    print(f"   íŒŒì¸íŠœë‹ BLEU: {ft_bleu:.2f}")
    print(f"   vs 4o-mini: +{ft_bleu - mini_bleu:.2f}ì  ({((ft_bleu - mini_bleu)/mini_bleu*100):+.1f}%)")
    print(f"   vs GPT-4o: +{ft_bleu - gpt4o_bleu:.2f}ì  ({((ft_bleu - gpt4o_bleu)/gpt4o_bleu*100):+.1f}%)")
    
    return df_detailed, final_results

# ========================================================================================
# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ========================================================================================

if __name__ == "__main__":
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        exit(1)
    
    print("ğŸš€ 3ê°œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {CSV_FILE_PATH}")
    print(f"ğŸ¯ íŒŒì¸íŠœë‹ ëª¨ë¸: {FINETUNED_MODEL}")
    print(f"ğŸ¤– GPT-4o-mini: {BASELINE_MINI_MODEL}")
    print(f"ğŸ”¥ GPT-4o: {BASELINE_4O_MODEL}")
    print(f"ğŸŒ¡ï¸ ì˜¨ë„ ì„¤ì •: {TEMPERATURE}")
    
    run_three_model_evaluation()
    
    print("\nâœ… 3ê°œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")