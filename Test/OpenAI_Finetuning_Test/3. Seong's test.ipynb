{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb641fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce2a09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>total_requirements</th>\n",
       "      <th>project_info</th>\n",
       "      <th>ERD_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'projectName': '스터디 그룹 성과 분석 도구', 'projectTa...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '스터디 그룹 성과 분석 도구', ...</td>\n",
       "      <td>{'erd_tables': [{'name': 'users', 'erd_columns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'projectName': '기억의 다리', 'projectTarget': '치...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '기억의 다리', 'category...</td>\n",
       "      <td>{'erd_tables': [{'name': 'Users', 'erd_columns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'projectName': '장애인 친화 대중교통 안내 앱', 'projectT...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '장애인 친화 대중교통 안내 앱',...</td>\n",
       "      <td>{'erd_tables': [{'name': 'Users', 'erd_columns...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  [{'projectName': '스터디 그룹 성과 분석 도구', 'projectTa...   \n",
       "1  [{'projectName': '기억의 다리', 'projectTarget': '치...   \n",
       "2  [{'projectName': '장애인 친화 대중교통 안내 앱', 'projectT...   \n",
       "\n",
       "                                  total_requirements  \\\n",
       "0  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "1  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "2  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "\n",
       "                                        project_info  \\\n",
       "0  {'project_info': {'title': '스터디 그룹 성과 분석 도구', ...   \n",
       "1  {'project_info': {'title': '기억의 다리', 'category...   \n",
       "2  {'project_info': {'title': '장애인 친화 대중교통 안내 앱',...   \n",
       "\n",
       "                                            ERD_data  \n",
       "0  {'erd_tables': [{'name': 'users', 'erd_columns...  \n",
       "1  {'erd_tables': [{'name': 'Users', 'erd_columns...  \n",
       "2  {'erd_tables': [{'name': 'Users', 'erd_columns...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"hehe.csv\")\n",
    "df.head(3)  # 앞부분만 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d55e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_field(field):\n",
    "    \"\"\"문자열로 저장된 dict/list를 실제 파이썬 객체로 변환\"\"\"\n",
    "    try:\n",
    "        return str(ast.literal_eval(field))\n",
    "    except Exception as e:\n",
    "        print(\"변환 오류:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a305f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://3.34.185.3:8000/\"\n",
    "\n",
    "def ERD_generate(user_info, requirement, project_info):\n",
    "    data = {\n",
    "        \"project_overview\": user_info,\n",
    "        \"requirements\": requirement,\n",
    "        \"project_summury\": project_info,\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.3,\n",
    "        \"model\": \"gpt-4o\"\n",
    "    }\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            url = base_url + \"api/PJA/json_ERD/generate\",\n",
    "            json = data,\n",
    "            headers= {\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        return response.json().get('json', None), elapsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"ERD_generate error:\", e)\n",
    "        return None, None\n",
    "\n",
    "def new_ERD_generate(user_info, requirement, project_info):\n",
    "    data = {\n",
    "        \"project_overview\": user_info,\n",
    "        \"requirements\": requirement,\n",
    "        \"project_summury\": project_info,\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.3,\n",
    "        \"model\": \"ft:gpt-4o-mini-2024-07-18:test:pja-erd-finetuning-model:BmOgyl6o\"\n",
    "    }\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            url = base_url + \"api/PJA/json_ERD/generate\",\n",
    "            json = data,\n",
    "            headers= {\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        return response.json().get('json', None), elapsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"new_ERD_generate error:\", e)\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64cd2a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o ERD 결과: {'erd_tables': [{'name': 'users', 'erd_columns': [{'name': 'user_id', 'data_type': 'serial', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'email', 'data_type': 'varchar(255)', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'password', 'data_type': 'varchar(255)', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'name', 'data_type': 'varchar(255)', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'email_verified', 'data_type': 'boolean', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}]}, {'name': 'study_groups', 'erd_columns': [{'name': 'group_id', 'data_type': 'serial', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'group_name', 'data_type': 'varchar(255)', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'created_by', 'data_type': 'integer', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}]}, {'name': 'quizzes', 'erd_columns': [{'name': 'quiz_id', 'data_type': 'serial', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'group_id', 'data_type': 'integer', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}, {'name': 'quiz_name', 'data_type': 'varchar(255)', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}]}, {'name': 'announcements', 'erd_columns': [{'name': 'announcement_id', 'data_type': 'serial', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'group_id', 'data_type': 'integer', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}, {'name': 'content', 'data_type': 'text', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'created_by', 'data_type': 'integer', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}]}, {'name': 'study_materials', 'erd_columns': [{'name': 'material_id', 'data_type': 'serial', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'group_id', 'data_type': 'integer', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}, {'name': 'file_name', 'data_type': 'varchar(255)', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'uploaded_by', 'data_type': 'integer', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}]}], 'erd_relationships': [{'from_table': 'study_groups', 'to_table': 'users', 'relationship_type': 'one-to-many', 'foreign_key': 'created_by', 'constraint_name': 'fk_study_groups_users'}, {'from_table': 'quizzes', 'to_table': 'study_groups', 'relationship_type': 'one-to-many', 'foreign_key': 'group_id', 'constraint_name': 'fk_quizzes_study_groups'}, {'from_table': 'announcements', 'to_table': 'study_groups', 'relationship_type': 'one-to-many', 'foreign_key': 'group_id', 'constraint_name': 'fk_announcements_study_groups'}, {'from_table': 'announcements', 'to_table': 'users', 'relationship_type': 'many-to-one', 'foreign_key': 'created_by', 'constraint_name': 'fk_announcements_users'}, {'from_table': 'study_materials', 'to_table': 'study_groups', 'relationship_type': 'one-to-many', 'foreign_key': 'group_id', 'constraint_name': 'fk_study_materials_study_groups'}, {'from_table': 'study_materials', 'to_table': 'users', 'relationship_type': 'many-to-one', 'foreign_key': 'uploaded_by', 'constraint_name': 'fk_study_materials_users'}]}\n",
      "GPT-4o 응답시간: 17.7692232131958 초\n",
      "Fine-tuned GPT-4o-mini ERD 결과: {'erd_tables': [{'name': '사용자', 'erd_columns': [{'name': '사용자ID', 'data_type': 'INTEGER', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': '이메일', 'data_type': 'VARCHAR', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': '비밀번호', 'data_type': 'VARCHAR', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}]}, {'name': '스터디 그룹', 'erd_columns': [{'name': '스터디ID', 'data_type': 'INTEGER', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': '스터디명', 'data_type': 'VARCHAR', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': '관리자ID', 'data_type': 'INTEGER', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}]}, {'name': '퀴즈', 'erd_columns': [{'name': '퀴즈ID', 'data_type': 'INTEGER', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': '제목', 'data_type': 'VARCHAR', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': '스터디ID', 'data_type': 'INTEGER', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}]}, {'name': '공지사항', 'erd_columns': [{'name': '공지ID', 'data_type': 'INTEGER', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': '내용', 'data_type': 'TEXT', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': '스터디ID', 'data_type': 'INTEGER', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}]}, {'name': '학습 자료', 'erd_columns': [{'name': '자료ID', 'data_type': 'INTEGER', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': '파일명', 'data_type': 'VARCHAR', 'is_primary_key': False, 'is_foreign_key': False, 'is_nullable': False}, {'name': '스터디ID', 'data_type': 'INTEGER', 'is_primary_key': False, 'is_foreign_key': True, 'is_nullable': False}]}], 'erd_relationships': [{'from_table': '사용자', 'to_table': '스터디 그룹', 'relationship_type': 'one-to-many', 'foreign_key': '관리자ID', 'constraint_name': 'fk_사용자_스터디그룹'}, {'from_table': '스터디 그룹', 'to_table': '퀴즈', 'relationship_type': 'one-to-many', 'foreign_key': '스터디ID', 'constraint_name': 'fk_스터디그룹_퀴즈'}, {'from_table': '스터디 그룹', 'to_table': '공지사항', 'relationship_type': 'one-to-many', 'foreign_key': '스터디ID', 'constraint_name': 'fk_스터디그룹_공지사항'}, {'from_table': '스터디 그룹', 'to_table': '학습 자료', 'relationship_type': 'one-to-many', 'foreign_key': '스터디ID', 'constraint_name': 'fk_스터디그룹_학습자료'}]}\n",
      "Fine-tuned 응답시간: 13.895011901855469 초\n"
     ]
    }
   ],
   "source": [
    "# 0번(첫 번째) 프로젝트 샘플로 테스트\n",
    "row = df.iloc[0]\n",
    "user_input = parse_field(row['user_input'])\n",
    "requirements = parse_field(row['total_requirements'])\n",
    "project_info = parse_field(row['project_info'])\n",
    "\n",
    "# 기본 모델 ERD\n",
    "erd1, time1 = ERD_generate(user_input, requirements, project_info)\n",
    "print(\"GPT-4o ERD 결과:\", erd1)\n",
    "print(\"GPT-4o 응답시간:\", time1, \"초\")\n",
    "\n",
    "# 파인튜닝 모델 ERD\n",
    "erd2, time2 = new_ERD_generate(user_input, requirements, project_info)\n",
    "print(\"Fine-tuned GPT-4o-mini ERD 결과:\", erd2)\n",
    "print(\"Fine-tuned 응답시간:\", time2, \"초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b1a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 스터디 그룹 성과 분석 도구 완료\n",
      "[2] 기억의 다리 완료\n",
      "[3] 장애인 친화 대중교통 안내 앱 완료\n",
      "[4] 가상 피팅룸 서비스 완료\n",
      "[5] 스마트 건강 관리 비서 완료\n",
      "[6] 그린 챌린지 완료\n",
      "[7] 스마트 학습 기록 관리자 완료\n",
      "[8] 스마트 영양소 분석기 완료\n",
      "[9] 감정 기록 매니저 완료\n",
      "[10] 재무관리 도우미(Financial Buddy) 완료\n",
      "[11] 결혼 준비 도우미 완료\n",
      "[12] 중고 물품 대여 매칭 플랫폼 완료\n",
      "[13] EcoSense: 실시간 환경 모니터링 시스템 완료\n",
      "[14] 스마트 공기질 분석 시스템 완료\n",
      "[15] 비접촉식 감정 인식 시스템 완료\n",
      "[16] 스마트 공기질 관리 시스템 완료\n",
      "[17] 필체 스타일 변환기 완료\n",
      "[18] 스마트 에너지 관리 시스템 완료\n",
      "[19] 손 제스처 기반 금융 보안 인증 시스템 완료\n",
      "[20] Smart Consumer Insight Platform 완료\n",
      "[21] 실시간 소비자 의견 분석 플랫폼 완료\n",
      "[22] 스마트 교통 관리 시스템 완료\n",
      "[23] 스마트 실내 공기 질 관리 시스템 완료\n",
      "[24] 약 복용 리마인더 및 모니터링 시스템 완료\n",
      "[25] 스마트 피트니스 코치 완료\n",
      "[26] 스마트 음성 안내 내비게이션 완료\n",
      "[27] 출장 스트레스 관리 플랫폼 완료\n",
      "[28] 축제 방문자 흐름 최적화 시스템 완료\n",
      "[29] 전통시장 스마트 재고 관리 시스템 완료\n",
      "[30] 스마트 출결 관리 시스템 완료\n",
      "[31] 스마트 에너지 모니터링 시스템 완료\n",
      "[32] UrbanGreen Monitor 완료\n",
      "[33] BookClub 완료\n",
      "[34] ExpenseAnalyzer 완료\n",
      "[35] MealPrepMaster 완료\n",
      "[36] EcoTrack 완료\n",
      "[37] 그린 케어: 스마트 식물 관리 솔루션 완료\n",
      "[38] SmartTask 도우미 완료\n",
      "[39] 리뷰북 커뮤니티 완료\n",
      "[40] StudyTracker 완료\n",
      "[41] 스마트학습관리 완료\n",
      "[42] 스마트 농업 분석기 완료\n",
      "[43] TeamSync 완료\n",
      "[44] 리뷰 진위 확인 시스템 완료\n",
      "[45] MedSchedule 완료\n",
      "[46] AI 고객 피드백 분석 시스템 완료\n",
      "[47] RecipeCollaborator 완료\n",
      "[48] 개인화 학습 경험 개선기 완료\n",
      "[49] 스마트 학생 성적 관리 시스템 완료\n",
      "[50] 스마트 일정 조정기 완료\n",
      "[51] 스마트 학습 계획 도구 완료\n",
      "[52] TeamSync 프로젝트 관리 시스템 완료\n",
      "[53] 스마트 에너지 관리 시스템 완료\n",
      "[54] TeamSync 완료\n",
      "[55] AgriConnect 완료\n",
      "[56] StudySphere 완료\n",
      "[57] 헤르도: 다국어 협업 플랫폼 완료\n",
      "[58] NutriTrack 완료\n",
      "[59] HabitTracker 완료\n",
      "[60] LocalFarmConnect 완료\n",
      "[61] TaskSync 완료\n",
      "[62] Ad Performance Analyzer 완료\n",
      "[63] LocalEventHub 완료\n",
      "[64] 누가나 파크Park 완료\n",
      "[65] GreenWaste Tracker 완료\n",
      "[66] 개인 맞춤형 학습 경로 설계기 완료\n",
      "[67] TeamConnect 완료\n",
      "[68] 열차 여행 마스터 완료\n",
      "[69] SmartRecipe 완료\n",
      "[70] DeliveryScheduler 완료\n",
      "[71] EcoChallenge 완료\n",
      "[72] EcoTrack 앱 완료\n",
      "[73] HabitSync 완료\n",
      "[74] SleepTrack 완료\n",
      "[75] MealPlanPro 완료\n",
      "[76] E-매너스: 온라인 회의 에티켓 교육 플랫폼 완료\n",
      "[77] TravelBuddy 일정 관리 시스템 완료\n",
      "[78] SmartRecipe 완료\n",
      "[79] 협력 업체 평가 및 비교 플랫폼 완료\n",
      "[80] WeatherAlert 앱 완료\n",
      "[81] 반려견 입양 매칭 플랫폼 완료\n",
      "[82] CafeOrderSync 완료\n",
      "[83] PriceWatch 완료\n",
      "[84] GroupLearn 완료\n",
      "[85] StockMaster 완료\n",
      "[86] LearnWise 완료\n",
      "[87] SmartDocSys 완료\n",
      "[88] FreshTrack 완료\n",
      "[89] TableTracker 완료\n",
      "[90] 헬스 트래커 커뮤니티 완료\n",
      "[91] 안전한 중고 거래 보조 플랫폼 완료\n",
      "[92] 명품 인증 및 거래 플랫폼 완료\n",
      "[93] Festive Planner 완료\n",
      "[94] 투두 캘린더 마스터 완료\n",
      "[95] PetCare Tracker 완료\n",
      "[96] LearnTrack 시스템 완료\n",
      "[97] 연참 프로젝트 Management 완료\n",
      "[98] TeamSync 완료\n",
      "[99] 언어 교환 네트워크 완료\n",
      "[100] LightOptimizer 완료\n",
      "[101] EcoShopper 완료\n",
      "[102] Book Review Aggregator 완료\n",
      "[103] 팀 협업 및 피드백 플랫폼 완료\n",
      "[104] 팀워크 플러스: 가상 팀 프로젝트 관리 도구 완료\n",
      "[105] StudyBuddy 멘토링 플랫폼 완료\n",
      "[106] 스마트 약물 관리 시스템 완료\n",
      "[107] BookCircle 완료\n",
      "[108] CodeReviewEnhancer 완료\n",
      "[109] MeetingOptimizer 완료\n",
      "[110] BookTrade 완료\n",
      "[111] 팀워크(TeamWork) 완료\n",
      "[112] 스마트 통근 최적화 플랫폼 완료\n",
      "[113] FocusPlan 완료\n",
      "[114] RecyClean 완료\n",
      "[115] 타임 마스터 완료\n",
      "[116] EcoTracker 완료\n",
      "[117] ScheduleSync 완료\n",
      "[118] 팀 협업 관리 플랫폼 완료\n",
      "[119] 뉴스 큐레이터: 맞춤형 뉴스 알림 서비스 완료\n",
      "[120] 큐레이드(CURADE) - 개인화된 콘텐츠 큐레이션 플랫폼 완료\n",
      "[121] StockTrack Pro 완료\n",
      "[122] Local Food Share 플랫폼 완료\n",
      "[123] 스터디 그룹 관리 시스템 완료\n",
      "[124] 스마트 레시피 매니저 완료\n",
      "[125] 가족 일정 공유 앱 완료\n",
      "[126] TaskCollab 완료\n",
      "[127] SmartScheduler 완료\n",
      "[128] 점심 360 완료\n",
      "[129] 소비 패턴 분석기 완료\n",
      "[130] 스마트 생산성 리포트 생성기 완료\n",
      "[131] StudyTracker 완료\n",
      "[132] TeamSync 업무 관리 플랫폼 완료\n",
      "[133] FocusMind 앱 완료\n",
      "[134] PersonaInsight 완료\n",
      "[135] EcoTrack 완료\n",
      "[136] 스터디 매니저 완료\n",
      "[137] 디지털 관리 코치 완료\n",
      "[138] LearnBuddy 완료\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_title</th>\n",
       "      <th>gpt-4o_erd</th>\n",
       "      <th>gpt-4o_time</th>\n",
       "      <th>finetuned_erd</th>\n",
       "      <th>finetuned_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스터디 그룹 성과 분석 도구</td>\n",
       "      <td>None</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>None</td>\n",
       "      <td>0.020827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>기억의 다리</td>\n",
       "      <td>None</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>None</td>\n",
       "      <td>0.022652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>장애인 친화 대중교통 안내 앱</td>\n",
       "      <td>None</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>None</td>\n",
       "      <td>0.026310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>가상 피팅룸 서비스</td>\n",
       "      <td>None</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>None</td>\n",
       "      <td>0.020398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스마트 건강 관리 비서</td>\n",
       "      <td>None</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>None</td>\n",
       "      <td>0.019620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      project_title gpt-4o_erd  gpt-4o_time finetuned_erd  finetuned_time\n",
       "0   스터디 그룹 성과 분석 도구       None     0.022335          None        0.020827\n",
       "1            기억의 다리       None     0.020424          None        0.022652\n",
       "2  장애인 친화 대중교통 안내 앱       None     0.019332          None        0.026310\n",
       "3        가상 피팅룸 서비스       None     0.015917          None        0.020398\n",
       "4      스마트 건강 관리 비서       None     0.018303          None        0.019620"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    user_input = parse_field(row['user_input'])\n",
    "    requirements = parse_field(row['total_requirements'])\n",
    "    project_info = parse_field(row['project_info'])\n",
    "\n",
    "    erd1, time1 = ERD_generate(user_input, requirements, project_info)\n",
    "    erd2, time2 = new_ERD_generate(user_input, requirements, project_info)\n",
    "\n",
    "    results.append({\n",
    "        \"project_title\": project_info['project_info']['title'],\n",
    "        \"gpt-4o_erd\": erd1,\n",
    "        \"gpt-4o_time\": time1,\n",
    "        \"finetuned_erd\": erd2,\n",
    "        \"finetuned_time\": time2\n",
    "    })\n",
    "    print(f\"[{idx+1}] {project_info['project_info']['title']} 완료\")\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c98b6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비교 결과가 erd_comparison_results.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "result_df.to_csv(\"erd_comparison_results.csv\", index=False)\n",
    "print(\"비교 결과가 erd_comparison_results.csv에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7b9f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o ERD Table 수: N/A\n",
      "Fine-tuned ERD Table 수: N/A\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 첫 번째 프로젝트 예시로 구조 비교\n",
    "print(\"GPT-4o ERD Table 수:\", len(erd1['erd_tables']) if erd1 else \"N/A\")\n",
    "print(\"Fine-tuned ERD Table 수:\", len(erd2['erd_tables']) if erd2 else \"N/A\")\n",
    "\n",
    "# (더 복잡한 비교/시각화는 이후 추가 가능)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8957739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_title</th>\n",
       "      <th>gpt-4o_erd</th>\n",
       "      <th>gpt-4o_time</th>\n",
       "      <th>finetuned_erd</th>\n",
       "      <th>finetuned_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스터디 그룹 성과 분석 도구</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>기억의 다리</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>장애인 친화 대중교통 안내 앱</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      project_title  gpt-4o_erd  gpt-4o_time  finetuned_erd  finetuned_time\n",
       "0   스터디 그룹 성과 분석 도구         NaN     0.022335            NaN        0.020827\n",
       "1            기억의 다리         NaN     0.020424            NaN        0.022652\n",
       "2  장애인 친화 대중교통 안내 앱         NaN     0.019332            NaN        0.026310"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"erd_comparison_results.csv\")\n",
    "df.head(3)  # 앞부분만 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de398f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "파인튜닝 모델 vs GPT-4o 성능 비교 테스트\n",
    "한국어 요구사항 분석 태스크 평가\n",
    "\n",
    "파일명: benchmark_pja_vs_gpt4o.py\n",
    "목적: 파인튜닝된 모델과 GPT-4o의 성능을 정량적으로 비교 분석\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "\n",
    "# ========================================================================================\n",
    "# 전역 설정 및 상수 정의\n",
    "# ========================================================================================\n",
    "\n",
    "# 테스트할 모델 정의\n",
    "FINETUNED_MODEL = \"ft:gpt-4o-mini-2024-07-18:test:pja-erd-finetuning-model:BmOgyl6o\"  # 파인튜닝된 모델\n",
    "BASELINE_MODEL = \"gpt-4o\"  # 비교 대상인 기본 GPT-4o 모델\n",
    "\n",
    "# API 호출 설정\n",
    "TEMPERATURE = 0.2  # 낮은 온도로 일관성 있는 응답 생성\n",
    "CSV_FILE_PATH = \"hehe.csv\"  # 테스트 데이터가 들어있는 CSV 파일 경로\n",
    "\n",
    "# 한국어 시스템 프롬프트 - 요구사항 분석 전문가 역할 정의\n",
    "SYSTEM_PROMPT = \"\"\"당신은 숙련된 소프트웨어 요구사항 분석 전문가입니다. \n",
    "사용자의 프로젝트 설명을 바탕으로 체계적이고 구체적인 요구사항 목록을 작성해주세요.\n",
    "기능적 요구사항과 성능 요구사항을 구분하여 작성하고, 각 요구사항은 명확하고 측정 가능해야 합니다.\"\"\"\n",
    "\n",
    "# ========================================================================================\n",
    "# 핵심 함수 정의\n",
    "# ========================================================================================\n",
    "\n",
    "def openai_chat_completion(model, user_input, project_info):\n",
    "    \"\"\"\n",
    "    OpenAI Chat Completion API를 호출하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model (str): 사용할 모델명 (파인튜닝 모델 또는 GPT-4o)\n",
    "        user_input (str): 사용자가 입력한 요구사항 설명\n",
    "        project_info (str): 프로젝트 정보 (JSON 형태의 문자열)\n",
    "    \n",
    "    Returns:\n",
    "        str: 모델이 생성한 요구사항 목록 텍스트\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 시스템 메시지와 사용자 메시지 구성\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "프로젝트 정보:\n",
    "{project_info}\n",
    "\n",
    "사용자 요구사항:\n",
    "{user_input}\n",
    "\n",
    "위 정보를 바탕으로 체계적인 요구사항 목록을 작성해주세요. \n",
    "기능적 요구사항과 성능 요구사항으로 구분하여 불릿 포인트 형태로 작성해주세요.\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        # OpenAI API 호출\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=TEMPERATURE,  # 일관성 있는 응답을 위한 낮은 온도\n",
    "            max_tokens=2000  # 충분한 길이의 응답을 위한 토큰 제한\n",
    "        )\n",
    "        \n",
    "        # 응답 텍스트 추출 및 공백 제거\n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # API 호출 실패시 에러 로그 출력 및 빈 문자열 반환\n",
    "        print(f\"API 호출 오류 ({model}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    모델 성능을 평가하는 다양한 메트릭을 계산하는 함수\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): 모델이 생성한 텍스트 목록\n",
    "        references (list): 정답(ground truth) 텍스트 목록\n",
    "    \n",
    "    Returns:\n",
    "        dict: 계산된 성능 메트릭들 (BLEU, ROUGE-L, 정확일치율)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. BLEU 점수 계산 - n-gram 기반 유사도 측정\n",
    "    # corpus_bleu는 전체 코퍼스에 대한 BLEU 점수를 계산\n",
    "    bleu_score = corpus_bleu(predictions, [references]).score\n",
    "    \n",
    "    # 2. ROUGE-L 점수 계산 - 최장 공통 부분 수열 기반 유사도 측정\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [scorer.score(ref, pred)['rougeL'].fmeasure \n",
    "                   for ref, pred in zip(references, predictions)]\n",
    "    rouge_l_score = np.mean(rouge_scores) * 100  # 백분율로 변환\n",
    "    \n",
    "    # 3. 정확 일치율 계산 - 완전히 동일한 출력의 비율\n",
    "    exact_matches = sum(1 for p, r in zip(predictions, references) \n",
    "                       if p.strip() == r.strip())\n",
    "    exact_match_rate = (exact_matches / len(references)) * 100\n",
    "    \n",
    "    # 결과를 딕셔너리로 반환 (소수점 둘째 자리까지)\n",
    "    return {\n",
    "        \"BLEU\": round(bleu_score, 2),\n",
    "        \"ROUGE-L\": round(rouge_l_score, 2),\n",
    "        \"정확일치율(%)\": round(exact_match_rate, 2)\n",
    "    }\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"\n",
    "    메인 평가 실행 함수 - 전체 성능 비교 프로세스를 관리\n",
    "    \"\"\"\n",
    "    print(\"📊 파인튜닝 모델 vs GPT-4o 성능 비교 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 1. 데이터 로드 및 검증\n",
    "    # ========================================================================================\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_FILE_PATH)\n",
    "        print(f\"✅ 데이터 로드 완료: {len(df)}개 샘플\")\n",
    "        \n",
    "        # 필수 컬럼 존재 여부 확인\n",
    "        required_columns = ['user_input', 'project_info', 'total_requirements']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"필수 컬럼이 누락되었습니다: {missing_columns}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CSV 파일 로드 실패: {e}\")\n",
    "        return\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 2. 모델별 추론 실행\n",
    "    # ========================================================================================\n",
    "    \n",
    "    # 각 모델의 결과를 저장할 딕셔너리 초기화\n",
    "    results = {\n",
    "        FINETUNED_MODEL: [],  # 파인튜닝 모델 결과\n",
    "        BASELINE_MODEL: []    # GPT-4o 결과\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n🔄 {len(df)}개 샘플에 대해 모델 추론 시작...\")\n",
    "    \n",
    "    # 각 데이터 샘플에 대해 두 모델로 추론 수행\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"모델 추론 진행\"):\n",
    "        user_input = row['user_input']        # 사용자 요구사항 설명\n",
    "        project_info = row['project_info']    # 프로젝트 정보\n",
    "        \n",
    "        # 파인튜닝 모델로 추론\n",
    "        print(f\"  📝 샘플 {idx+1}: 파인튜닝 모델 추론 중...\")\n",
    "        ft_result = openai_chat_completion(FINETUNED_MODEL, user_input, project_info)\n",
    "        results[FINETUNED_MODEL].append(ft_result)\n",
    "        \n",
    "        # GPT-4o 모델로 추론  \n",
    "        print(f\"  📝 샘플 {idx+1}: GPT-4o 추론 중...\")\n",
    "        gpt4o_result = openai_chat_completion(BASELINE_MODEL, user_input, project_info)\n",
    "        results[BASELINE_MODEL].append(gpt4o_result)\n",
    "        \n",
    "        # API 호출 간격 조정 (레이트 리미트 방지)\n",
    "        # 0.5초 대기로 안정적인 API 호출 보장\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # 진행 상황 출력 (10개마다)\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  ✅ {idx + 1}/{len(df)} 샘플 완료\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 3. 성능 메트릭 계산 및 결과 출력\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(\"\\n📈 성능 평가 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 정답 데이터 (ground truth) 추출\n",
    "    references = df['ERD_data'].tolist()\n",
    "    \n",
    "    # 각 모델별로 성능 메트릭 계산\n",
    "    final_results = {}\n",
    "    for model_name, predictions in results.items():\n",
    "        # 모델명을 한국어로 표시하기 위한 변환\n",
    "        model_display_name = \"파인튜닝 모델\" if \"ft:\" in model_name else \"GPT-4o\"\n",
    "        \n",
    "        # 성능 메트릭 계산\n",
    "        metrics = calculate_metrics(predictions, references)\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"\\n🤖 {model_display_name}\")\n",
    "        print(f\"   BLEU 점수: {metrics['BLEU']}\")\n",
    "        print(f\"   ROUGE-L 점수: {metrics['ROUGE-L']}\")\n",
    "        print(f\"   정확 일치율: {metrics['정확일치율(%)']}%\")\n",
    "        \n",
    "        # 최종 결과 딕셔너리에 저장\n",
    "        model_key = \"파인튜닝_모델\" if \"ft:\" in model_name else \"GPT-4o\"\n",
    "        final_results[model_key] = metrics\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 4. 결과 저장\n",
    "    # ========================================================================================\n",
    "    \n",
    "    # JSON 형태로 성능 비교 결과 저장\n",
    "    try:\n",
    "        with open(\"성능비교_결과.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n💾 결과가 '성능비교_결과.json' 파일로 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 결과 저장 실패: {e}\")\n",
    "    \n",
    "    # 상세한 출력 결과를 CSV로 저장\n",
    "    try:\n",
    "        output_df = df.copy()\n",
    "        output_df['파인튜닝_모델_출력'] = results[FINETUNED_MODEL]\n",
    "        output_df['GPT4o_출력'] = results[BASELINE_MODEL]\n",
    "        output_df.to_csv(\"모델_출력_비교.csv\", index=False, encoding=\"utf-8\")\n",
    "        print(f\"📄 상세 출력 결과가 '모델_출력_비교.csv' 파일로 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 상세 결과 저장 실패: {e}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 5. 성능 차이 분석\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n📊 성능 차이 분석\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(final_results) == 2:\n",
    "        ft_bleu = final_results['파인튜닝_모델']['BLEU']\n",
    "        gpt4o_bleu = final_results['GPT-4o']['BLEU']\n",
    "        bleu_diff = ft_bleu - gpt4o_bleu\n",
    "        \n",
    "        ft_rouge = final_results['파인튜닝_모델']['ROUGE-L']\n",
    "        gpt4o_rouge = final_results['GPT-4o']['ROUGE-L']\n",
    "        rouge_diff = ft_rouge - gpt4o_rouge\n",
    "        \n",
    "        print(f\"BLEU 점수 차이: {bleu_diff:+.2f} ({'향상' if bleu_diff > 0 else '저하'})\")\n",
    "        print(f\"ROUGE-L 점수 차이: {rouge_diff:+.2f} ({'향상' if rouge_diff > 0 else '저하'})\")\n",
    "        \n",
    "        # 성능 개선 여부 판단\n",
    "        if bleu_diff > 0 and rouge_diff > 0:\n",
    "            print(\"🎉 파인튜닝 모델이 모든 지표에서 우수한 성능을 보입니다!\")\n",
    "        elif bleu_diff < 0 and rouge_diff < 0:\n",
    "            print(\"⚠️  파인튜닝 모델의 성능이 기본 모델보다 낮습니다.\")\n",
    "        else:\n",
    "            print(\"📝 일부 지표에서만 성능 향상이 있습니다.\")\n",
    "\n",
    "# ========================================================================================\n",
    "# 메인 실행 부분\n",
    "# ========================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # OpenAI API 키 확인\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"❌ OPENAI_API_KEY 환경변수를 설정해주세요.\")\n",
    "        print(\"   export OPENAI_API_KEY='your-api-key-here'\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"🚀 성능 비교 테스트를 시작합니다...\")\n",
    "    print(f\"📁 테스트 데이터: {CSV_FILE_PATH}\")\n",
    "    print(f\"🤖 파인튜닝 모델: {FINETUNED_MODEL}\")\n",
    "    print(f\"🤖 기본 모델: {BASELINE_MODEL}\")\n",
    "    print(f\"🌡️  온도 설정: {TEMPERATURE}\")\n",
    "    \n",
    "    # 메인 평가 함수 실행\n",
    "    run_evaluation()\n",
    "    \n",
    "    print(\"\\n✅ 성능 비교 테스트가 완료되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9661fe58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2eae87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 개선된 성능 비교 테스트를 시작합니다...\n",
      "📁 테스트 데이터: hehe.csv\n",
      "🤖 파인튜닝 모델: ft:gpt-4o-mini-2024-07-18:test:pja-erd-finetuning-model:BmOgyrDW:ckpt-step-124\n",
      "🤖 기본 모델: gpt-4o\n",
      "🌡️ 온도 설정: 0.2\n",
      "📊 파인튜닝 모델 vs GPT-4o 성능 비교 시작 (개선 버전)\n",
      "============================================================\n",
      "✅ 데이터 로드 완료: 138개 샘플\n",
      "📋 CSV 컬럼: ['user_input', 'total_requirements', 'project_info', 'ERD_data']\n",
      "\n",
      "🔄 데이터 전처리 중...\n",
      "\n",
      "📋 데이터 샘플 확인:\n",
      "\n",
      "샘플 1:\n",
      "  user_input: <class 'list'> - [{'projectName': '스터디 그룹 성과 분석 도구', 'projectTarget': '학습자 및 교육자', 'mainFunction': ['스터디 그룹 성과 시각화', ...\n",
      "  total_requirements: <class 'list'> - [{'requirementType': 'FUNCTIONAL', 'content': '사용자는 회원가입 시 이메일 인증을 통해 계정 생성의 안전성을 높일 수 있어야 한다.'}, {'...\n",
      "  project_info: <class 'dict'> - {'project_info': {'title': '스터디 그룹 성과 분석 도구', 'category': '웹앱', 'target_users': ['학습자', '교육자', '스터디 ...\n",
      "  ERD_data: <class 'dict'> - {'erd_tables': [{'name': 'users', 'erd_columns': [{'name': 'user_id', 'data_type': 'SERIAL', 'is_pri...\n",
      "\n",
      "샘플 2:\n",
      "  user_input: <class 'list'> - [{'projectName': '기억의 다리', 'projectTarget': '치매 환자와 그 가족', 'mainFunction': ['추억 사진 공유 및 댓글 기능', '과거 ...\n",
      "  total_requirements: <class 'list'> - [{'requirementType': 'FUNCTIONAL', 'content': '사용자는 최대 10장의 사진을 한 번에 업로드 할 수 있어야 하며, 각 사진에 대해 제목과 설명...\n",
      "  project_info: <class 'dict'> - {'project_info': {'title': '기억의 다리', 'category': '웹앱', 'target_users': ['치매 환자', '치매 환자의 가족', '간병인',...\n",
      "  ERD_data: <class 'dict'> - {'erd_tables': [{'name': 'Users', 'erd_columns': [{'name': 'user_id', 'data_type': 'ObjectId', 'is_p...\n",
      "\n",
      "🔄 5개 샘플에 대해 모델 추론 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 1 처리 중...\n",
      "   입력 타입: <class 'list'>\n",
      "   프로젝트 정보 타입: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  20%|██        | 1/5 [00:37<02:30, 37.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 2 처리 중...\n",
      "   입력 타입: <class 'list'>\n",
      "   프로젝트 정보 타입: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  40%|████      | 2/5 [01:15<01:52, 37.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 3 처리 중...\n",
      "   입력 타입: <class 'list'>\n",
      "   프로젝트 정보 타입: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  60%|██████    | 3/5 [01:47<01:10, 35.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 4 처리 중...\n",
      "   입력 타입: <class 'list'>\n",
      "   프로젝트 정보 타입: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  80%|████████  | 4/5 [02:30<00:38, 38.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 5 처리 중...\n",
      "   입력 타입: <class 'list'>\n",
      "   프로젝트 정보 타입: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행: 100%|██████████| 5/5 [03:10<00:00, 38.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 성능 평가 결과\n",
      "============================================================\n",
      "📊 정답 데이터 형태 확인:\n",
      "  정답 1: <class 'dict'> - {'erd_tables': [{'name': 'users', 'erd_columns': [{'name': 'user_id', 'data_type': 'SERIAL', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'email', 'data_type': 'VAR...\n",
      "  정답 2: <class 'dict'> - {'erd_tables': [{'name': 'Users', 'erd_columns': [{'name': 'user_id', 'data_type': 'ObjectId', 'is_primary_key': True, 'is_foreign_key': False, 'is_nullable': False}, {'name': 'username', 'data_type':...\n",
      "\n",
      "🔍 파인튜닝 모델 출력 확인:\n",
      "  출력 1: 아래는 \"스터디 그룹 성과 분석 도구\" 프로젝트에 대한 ERD(Entity Relationship Diagram) 정보를 JSON 형태로 제공한 것입니다. 이 ERD는 사용자, 스터디 그룹, 퀴즈, 과제, 공지사항 등과 관련된 테이블 구조와 관계를 포함하고 있습니다.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"...\n",
      "  출력 2: 아래는 \"기억의 다리\" 프로젝트를 위한 ERD(Entity Relationship Diagram) 데이터입니다. 이 ERD는 사용자의 요구사항을 기반으로 테이블 구조, 컬럼 정보, 그리고 관계를 포함하고 있습니다.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"Users\",\n",
      "      \"columns\": [\n",
      "   ...\n",
      "\n",
      "🤖 파인튜닝 모델 성능:\n",
      "   BLEU: 69.09\n",
      "   ROUGE-L: 81.11\n",
      "   정확일치율(%): 0.0\n",
      "   의미적_유사도(%): 64.95\n",
      "\n",
      "🔍 GPT-4o 출력 확인:\n",
      "  출력 1: 아래는 \"스터디 그룹 성과 분석 도구\" 프로젝트를 위한 ERD 데이터입니다. 이 데이터는 프로젝트 설명과 요구사항을 바탕으로 설계되었습니다. 각 테이블은 프로젝트의 핵심 기능을 지원하기 위해 설계되었으며, 테이블 간의 관계도 정의되어 있습니다.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"User\",\n",
      "      \"c...\n",
      "  출력 2: ```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Users\",\n",
      "      \"columns\": [\n",
      "        {\n",
      "          \"name\": \"user_id\",\n",
      "          \"type\": \"ObjectId\",\n",
      "          \"primary_key\": true\n",
      "        },\n",
      "        {\n",
      "      ...\n",
      "\n",
      "🤖 GPT-4o 성능:\n",
      "   BLEU: 41.29\n",
      "   ROUGE-L: 36.56\n",
      "   정확일치율(%): 0.0\n",
      "   의미적_유사도(%): 71.65\n",
      "\n",
      "💾 결과가 '성능비교_결과_개선.json' 파일로 저장되었습니다.\n",
      "📄 상세 출력이 '모델_출력_비교_개선.csv'로 저장되었습니다.\n",
      "\n",
      "📊 성능 차이 분석\n",
      "============================================================\n",
      "BLEU 차이: +27.80 (향상)\n",
      "ROUGE-L 차이: +44.55 (향상)\n",
      "의미적_유사도(%) 차이: -6.70 (저하)\n",
      "🎉 파인튜닝 모델이 전반적으로 우수한 성능을 보입니다!\n",
      "\n",
      "✅ 개선된 성능 비교 테스트가 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "파인튜닝 모델 vs GPT-4o 성능 비교 테스트 (개선 버전)\n",
    "한국어 요구사항 분석 태스크 평가\n",
    "\n",
    "주요 개선사항:\n",
    "1. 정답 데이터 참조 오류 수정 (ERD_data -> total_requirements)\n",
    "2. JSON 형태 데이터 처리 로직 추가\n",
    "3. 더 적합한 평가 메트릭 추가 (의미적 유사도)\n",
    "4. 에러 처리 강화\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ========================================================================================\n",
    "# 전역 설정 및 상수 정의\n",
    "# ========================================================================================\n",
    "\n",
    "# 테스트할 모델 정의\n",
    "FINETUNED_MODEL = \"ft:gpt-4o-mini-2024-07-18:test:pja-erd-finetuning-model:BmOgyrDW:ckpt-step-124\"\n",
    "BASELINE_MODEL = \"gpt-4o\"\n",
    "\n",
    "# API 호출 설정\n",
    "TEMPERATURE = 0.2\n",
    "CSV_FILE_PATH = \"hehe.csv\"\n",
    "\n",
    "# 한국어 ERD 생성 전문가 시스템 프롬프트 (파인튜닝 목적에 맞게 수정)\n",
    "SYSTEM_PROMPT = \"\"\"당신은 숙련된 데이터베이스 설계 전문가입니다. \n",
    "사용자의 프로젝트 설명과 요구사항을 바탕으로 ERD(Entity Relationship Diagram) 데이터를 생성해주세요.\n",
    "각 테이블의 구조, 컬럼 정보, 관계를 명확하게 정의해야 합니다.\n",
    "JSON 형태로 구조화된 ERD 정보를 제공해주세요.\"\"\"\n",
    "\n",
    "# 의미적 유사도 계산을 위한 모델 (한국어 지원)\n",
    "try:\n",
    "    semantic_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    SEMANTIC_SIMILARITY_AVAILABLE = True\n",
    "except:\n",
    "    print(\"⚠️ 의미적 유사도 모델을 로드할 수 없습니다. 해당 메트릭은 제외됩니다.\")\n",
    "    SEMANTIC_SIMILARITY_AVAILABLE = False\n",
    "\n",
    "# ========================================================================================\n",
    "# 유틸리티 함수\n",
    "# ========================================================================================\n",
    "\n",
    "def safe_parse_json_string(json_str):\n",
    "    \"\"\"\n",
    "    JSON 문자열을 안전하게 파싱하는 함수\n",
    "    \"\"\"\n",
    "    if pd.isna(json_str) or json_str == '':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 문자열이 이미 딕셔너리/리스트인 경우\n",
    "        if isinstance(json_str, (dict, list)):\n",
    "            return json_str\n",
    "        \n",
    "        # 문자열을 JSON으로 파싱 시도\n",
    "        if isinstance(json_str, str):\n",
    "            # ast.literal_eval 먼저 시도 (Python 리터럴)\n",
    "            try:\n",
    "                return ast.literal_eval(json_str)\n",
    "            except:\n",
    "                # json.loads 시도\n",
    "                return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON 파싱 오류: {e}\")\n",
    "        return json_str  # 원본 문자열 반환\n",
    "\n",
    "def format_data_for_prompt(data):\n",
    "    \"\"\"\n",
    "    데이터를 프롬프트에 적합한 형태로 포맷팅\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    elif isinstance(data, (dict, list)):\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        return str(data)\n",
    "\n",
    "def extract_text_content(data):\n",
    "    \"\"\"\n",
    "    JSON 데이터에서 텍스트 내용을 추출하여 비교 가능한 형태로 변환\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data.strip()\n",
    "    elif isinstance(data, dict):\n",
    "        # 딕셔너리에서 주요 텍스트 내용 추출\n",
    "        text_parts = []\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str):\n",
    "                text_parts.append(f\"{key}: {value}\")\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, str):\n",
    "                        text_parts.append(item)\n",
    "                    elif isinstance(item, dict):\n",
    "                        text_parts.append(json.dumps(item, ensure_ascii=False))\n",
    "        return \" \".join(text_parts)\n",
    "    elif isinstance(data, list):\n",
    "        # 리스트에서 텍스트 내용 추출\n",
    "        text_parts = []\n",
    "        for item in data:\n",
    "            if isinstance(item, str):\n",
    "                text_parts.append(item)\n",
    "            elif isinstance(item, dict):\n",
    "                text_parts.append(json.dumps(item, ensure_ascii=False))\n",
    "        return \" \".join(text_parts)\n",
    "    else:\n",
    "        return str(data)\n",
    "\n",
    "# ========================================================================================\n",
    "# 핵심 함수 정의\n",
    "# ========================================================================================\n",
    "\n",
    "def openai_chat_completion(model, user_input, project_info):\n",
    "    \"\"\"\n",
    "    OpenAI Chat Completion API를 호출하는 함수\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 데이터를 프롬프트 형태로 포맷팅\n",
    "        formatted_project_info = format_data_for_prompt(project_info)\n",
    "        formatted_user_input = format_data_for_prompt(user_input)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "프로젝트 정보:\n",
    "{formatted_project_info}\n",
    "\n",
    "요구사항:\n",
    "{formatted_user_input}\n",
    "\n",
    "위 정보를 바탕으로 ERD 데이터를 생성해주세요.\n",
    "테이블 구조, 컬럼 정보, 관계를 포함한 JSON 형태로 제공해주세요.\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"API 호출 오류 ({model}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_semantic_similarity(predictions, references):\n",
    "    \"\"\"\n",
    "    의미적 유사도를 계산하는 함수\n",
    "    \"\"\"\n",
    "    if not SEMANTIC_SIMILARITY_AVAILABLE:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        # 텍스트 내용 추출\n",
    "        pred_texts = [extract_text_content(pred) for pred in predictions]\n",
    "        ref_texts = [extract_text_content(ref) for ref in references]\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        pred_embeddings = semantic_model.encode(pred_texts)\n",
    "        ref_embeddings = semantic_model.encode(ref_texts)\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        similarities = []\n",
    "        for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):\n",
    "            similarity = cosine_similarity([pred_emb], [ref_emb])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        return np.mean(similarities) * 100\n",
    "    except Exception as e:\n",
    "        print(f\"의미적 유사도 계산 오류: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    모델 성능을 평가하는 다양한 메트릭을 계산하는 함수\n",
    "    \"\"\"\n",
    "    # 텍스트 내용 추출\n",
    "    pred_texts = [extract_text_content(pred) for pred in predictions]\n",
    "    ref_texts = [extract_text_content(ref) for ref in references]\n",
    "    \n",
    "    # 빈 텍스트 필터링\n",
    "    valid_pairs = [(p, r) for p, r in zip(pred_texts, ref_texts) if p.strip() and r.strip()]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        return {\n",
    "            \"BLEU\": 0.0,\n",
    "            \"ROUGE-L\": 0.0,\n",
    "            \"정확일치율(%)\": 0.0,\n",
    "            \"의미적_유사도(%)\": 0.0\n",
    "        }\n",
    "    \n",
    "    valid_preds, valid_refs = zip(*valid_pairs)\n",
    "    \n",
    "    # 1. BLEU 점수 계산\n",
    "    try:\n",
    "        bleu_score = corpus_bleu(list(valid_preds), [list(valid_refs)]).score\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    \n",
    "    # 2. ROUGE-L 점수 계산\n",
    "    try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        rouge_scores = [scorer.score(ref, pred)['rougeL'].fmeasure \n",
    "                       for ref, pred in zip(valid_refs, valid_preds)]\n",
    "        rouge_l_score = np.mean(rouge_scores) * 100\n",
    "    except:\n",
    "        rouge_l_score = 0.0\n",
    "    \n",
    "    # 3. 정확 일치율 계산\n",
    "    exact_matches = sum(1 for p, r in zip(valid_preds, valid_refs) \n",
    "                       if p.strip() == r.strip())\n",
    "    exact_match_rate = (exact_matches / len(valid_pairs)) * 100\n",
    "    \n",
    "    # 4. 의미적 유사도 계산\n",
    "    semantic_similarity = calculate_semantic_similarity(predictions, references)\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": round(bleu_score, 2),\n",
    "        \"ROUGE-L\": round(rouge_l_score, 2),\n",
    "        \"정확일치율(%)\": round(exact_match_rate, 2),\n",
    "        \"의미적_유사도(%)\": round(semantic_similarity, 2)\n",
    "    }\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"\n",
    "    메인 평가 실행 함수\n",
    "    \"\"\"\n",
    "    print(\"📊 파인튜닝 모델 vs GPT-4o 성능 비교 시작 (개선 버전)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 1. 데이터 로드 및 검증\n",
    "    # ========================================================================================\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_FILE_PATH)\n",
    "        print(f\"✅ 데이터 로드 완료: {len(df)}개 샘플\")\n",
    "        \n",
    "        # 컬럼 정보 출력\n",
    "        print(f\"📋 CSV 컬럼: {list(df.columns)}\")\n",
    "        \n",
    "        # 필수 컬럼 확인 (수정된 버전)\n",
    "        required_columns = ['user_input', 'project_info', 'total_requirements', 'ERD_data']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"⚠️ 일부 컬럼이 누락되었습니다: {missing_columns}\")\n",
    "            print(\"사용 가능한 컬럼으로 진행합니다.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CSV 파일 로드 실패: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 데이터 전처리\n",
    "    print(\"\\n🔄 데이터 전처리 중...\")\n",
    "    \n",
    "    # JSON 문자열을 파싱\n",
    "    for col in ['user_input', 'project_info', 'total_requirements', 'ERD_data']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(safe_parse_json_string)\n",
    "    \n",
    "    # 처음 몇 개 샘플의 데이터 형태 확인\n",
    "    print(\"\\n📋 데이터 샘플 확인:\")\n",
    "    for i, row in df.head(2).iterrows():\n",
    "        print(f\"\\n샘플 {i+1}:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {type(row[col])} - {str(row[col])[:100]}...\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 2. 모델별 추론 실행 (소규모 테스트)\n",
    "    # ========================================================================================\n",
    "    \n",
    "    # 테스트할 샘플 수 제한 (비용 절약)\n",
    "    test_samples = min(5, len(df))  # 최대 5개 샘플로 테스트\n",
    "    test_df = df.head(test_samples).copy()\n",
    "    \n",
    "    print(f\"\\n🔄 {test_samples}개 샘플에 대해 모델 추론 시작...\")\n",
    "    \n",
    "    results = {\n",
    "        FINETUNED_MODEL: [],\n",
    "        BASELINE_MODEL: []\n",
    "    }\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"모델 추론 진행\"):\n",
    "        user_input = row['user_input']\n",
    "        project_info = row['project_info']\n",
    "        \n",
    "        print(f\"\\n📝 샘플 {idx+1} 처리 중...\")\n",
    "        print(f\"   입력 타입: {type(user_input)}\")\n",
    "        print(f\"   프로젝트 정보 타입: {type(project_info)}\")\n",
    "        \n",
    "        # 파인튜닝 모델 추론\n",
    "        ft_result = openai_chat_completion(FINETUNED_MODEL, user_input, project_info)\n",
    "        results[FINETUNED_MODEL].append(ft_result)\n",
    "        \n",
    "        # GPT-4o 추론\n",
    "        gpt4o_result = openai_chat_completion(BASELINE_MODEL, user_input, project_info)\n",
    "        results[BASELINE_MODEL].append(gpt4o_result)\n",
    "        \n",
    "        time.sleep(1.0)  # API 레이트 리미트 방지\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 3. 성능 메트릭 계산\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(\"\\n📈 성능 평가 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 정답 데이터 - ERD_data를 정답으로 사용 (파인튜닝 목적에 맞게)\n",
    "    references = test_df['ERD_data'].tolist()\n",
    "    \n",
    "    print(f\"📊 정답 데이터 형태 확인:\")\n",
    "    for i, ref in enumerate(references[:2]):\n",
    "        print(f\"  정답 {i+1}: {type(ref)} - {str(ref)[:200]}...\")\n",
    "    \n",
    "    final_results = {}\n",
    "    for model_name, predictions in results.items():\n",
    "        model_display_name = \"파인튜닝 모델\" if \"ft:\" in model_name else \"GPT-4o\"\n",
    "        \n",
    "        print(f\"\\n🔍 {model_display_name} 출력 확인:\")\n",
    "        for i, pred in enumerate(predictions[:2]):\n",
    "            print(f\"  출력 {i+1}: {str(pred)[:200]}...\")\n",
    "        \n",
    "        metrics = calculate_metrics(predictions, references)\n",
    "        \n",
    "        print(f\"\\n🤖 {model_display_name} 성능:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"   {metric_name}: {value}\")\n",
    "        \n",
    "        model_key = \"파인튜닝_모델\" if \"ft:\" in model_name else \"GPT-4o\"\n",
    "        final_results[model_key] = metrics\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 4. 결과 저장 및 분석\n",
    "    # ========================================================================================\n",
    "    \n",
    "    # 결과 저장\n",
    "    try:\n",
    "        with open(\"성능비교_결과_개선.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n💾 결과가 '성능비교_결과_개선.json' 파일로 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 결과 저장 실패: {e}\")\n",
    "    \n",
    "    # 상세 결과 저장\n",
    "    try:\n",
    "        output_df = test_df.copy()\n",
    "        output_df['파인튜닝_모델_출력'] = results[FINETUNED_MODEL]\n",
    "        output_df['GPT4o_출력'] = results[BASELINE_MODEL]\n",
    "        output_df.to_csv(\"모델_출력_비교_개선.csv\", index=False, encoding=\"utf-8\")\n",
    "        print(f\"📄 상세 출력이 '모델_출력_비교_개선.csv'로 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 상세 결과 저장 실패: {e}\")\n",
    "    \n",
    "    # 성능 차이 분석\n",
    "    if len(final_results) == 2:\n",
    "        print(f\"\\n📊 성능 차이 분석\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        ft_metrics = final_results['파인튜닝_모델']\n",
    "        gpt4o_metrics = final_results['GPT-4o']\n",
    "        \n",
    "        for metric in ['BLEU', 'ROUGE-L', '의미적_유사도(%)']:\n",
    "            if metric in ft_metrics and metric in gpt4o_metrics:\n",
    "                diff = ft_metrics[metric] - gpt4o_metrics[metric]\n",
    "                print(f\"{metric} 차이: {diff:+.2f} ({'향상' if diff > 0 else '저하'})\")\n",
    "        \n",
    "        # 종합 평가\n",
    "        improvements = sum(1 for metric in ['BLEU', 'ROUGE-L', '의미적_유사도(%)'] \n",
    "                          if ft_metrics.get(metric, 0) > gpt4o_metrics.get(metric, 0))\n",
    "        \n",
    "        if improvements >= 2:\n",
    "            print(\"🎉 파인튜닝 모델이 전반적으로 우수한 성능을 보입니다!\")\n",
    "        elif improvements == 1:\n",
    "            print(\"📝 일부 지표에서 성능 향상이 있습니다.\")\n",
    "        else:\n",
    "            print(\"⚠️ 파인튜닝 효과가 제한적입니다. 데이터나 방법론 재검토가 필요할 수 있습니다.\")\n",
    "\n",
    "# ========================================================================================\n",
    "# 메인 실행 부분\n",
    "# ========================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"❌ OPENAI_API_KEY 환경변수를 설정해주세요.\")\n",
    "        print(\"   export OPENAI_API_KEY='your-api-key-here'\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"🚀 개선된 성능 비교 테스트를 시작합니다...\")\n",
    "    print(f\"📁 테스트 데이터: {CSV_FILE_PATH}\")\n",
    "    print(f\"🤖 파인튜닝 모델: {FINETUNED_MODEL}\")\n",
    "    print(f\"🤖 기본 모델: {BASELINE_MODEL}\")\n",
    "    print(f\"🌡️ 온도 설정: {TEMPERATURE}\")\n",
    "    \n",
    "    run_evaluation()\n",
    "    \n",
    "    print(\"\\n✅ 개선된 성능 비교 테스트가 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0829e836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0627 16:55:04.859000 24772 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "c:\\Users\\mir96\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 3개 모델 성능 비교 테스트를 시작합니다...\n",
      "📁 테스트 데이터: hehe.csv\n",
      "🎯 파인튜닝 모델: ft:gpt-4o-mini-2024-07-18:test:pja-erd-finetuning-model:BmOgyrDW:ckpt-step-124\n",
      "🤖 GPT-4o-mini: gpt-4o-mini\n",
      "🔥 GPT-4o: gpt-4o\n",
      "🌡️ 온도 설정: 0.2\n",
      "📊 3개 모델 성능 비교: 파인튜닝 vs GPT-4o-mini vs GPT-4o\n",
      "================================================================================\n",
      "✅ 데이터 로드 완료: 138개 샘플\n",
      "📋 CSV 컬럼: ['user_input', 'total_requirements', 'project_info', 'ERD_data']\n",
      "\n",
      "🔄 데이터 전처리 중...\n",
      "\n",
      "🔄 5개 샘플에 대해 3개 모델 추론 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 1 처리 중...\n",
      "   🎯 파인튜닝 모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  20%|██        | 1/5 [01:02<04:09, 62.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 2 처리 중...\n",
      "   🎯 파인튜닝 모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  40%|████      | 2/5 [02:03<03:04, 61.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 3 처리 중...\n",
      "   🎯 파인튜닝 모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  60%|██████    | 3/5 [02:52<01:52, 56.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 4 처리 중...\n",
      "   🎯 파인튜닝 모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  80%|████████  | 4/5 [03:42<00:53, 53.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 5 처리 중...\n",
      "   🎯 파인튜닝 모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행: 100%|██████████| 5/5 [04:33<00:00, 54.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 3개 모델 성능 평가 결과\n",
      "================================================================================\n",
      "\n",
      "🔍 파인튜닝_모델 출력 확인:\n",
      "  출력 1: {\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"사용자\",\n",
      "      \"erd_columns\": [\n",
      "        {\n",
      "          \"name\": \"사용자ID\",\n",
      "          \"data_type\": \"INTEGER\",\n",
      "         ...\n",
      "  출력 2: {\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"사용자\",\n",
      "      \"erd_columns\": [\n",
      "        {\n",
      "          \"name\": \"사용자ID\",\n",
      "          \"data_type\": \"ObjectId\",\n",
      "        ...\n",
      "\n",
      "🤖 파인튜닝_모델 성능:\n",
      "   BLEU: 75.17\n",
      "   ROUGE-L: 79.73\n",
      "   정확일치율(%): 0.0\n",
      "   의미적_유사도(%): 94.29\n",
      "\n",
      "🔍 GPT-4o-mini 출력 확인:\n",
      "  출력 1: {\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"users\",\n",
      "      \"erd_columns\": [\n",
      "        {\n",
      "          \"name\": \"user_id\",\n",
      "          \"data_type\": \"SERIAL\",\n",
      "      ...\n",
      "  출력 2: {\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"Users\",\n",
      "      \"erd_columns\": [\n",
      "        {\n",
      "          \"name\": \"user_id\",\n",
      "          \"data_type\": \"ObjectId\",\n",
      "    ...\n",
      "\n",
      "🤖 GPT-4o-mini 성능:\n",
      "   BLEU: 84.07\n",
      "   ROUGE-L: 84.61\n",
      "   정확일치율(%): 0.0\n",
      "   의미적_유사도(%): 95.98\n",
      "\n",
      "🔍 GPT-4o 출력 확인:\n",
      "  출력 1: {\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"users\",\n",
      "      \"erd_columns\": [\n",
      "        {\n",
      "          \"name\": \"user_id\",\n",
      "          \"data_type\": \"serial\",\n",
      "      ...\n",
      "  출력 2: {\n",
      "  \"erd_tables\": [\n",
      "    {\n",
      "      \"name\": \"Users\",\n",
      "      \"erd_columns\": [\n",
      "        {\n",
      "          \"name\": \"user_id\",\n",
      "          \"data_type\": \"ObjectId\",\n",
      "    ...\n",
      "\n",
      "🤖 GPT-4o 성능:\n",
      "   BLEU: 78.05\n",
      "   ROUGE-L: 81.41\n",
      "   정확일치율(%): 0.0\n",
      "   의미적_유사도(%): 95.59\n",
      "\n",
      "📊 종합 성능 비교 분석\n",
      "================================================================================\n",
      "메트릭             파인튜닝         4o-mini      GPT-4o       vs mini      vs 4o       \n",
      "--------------------------------------------------------------------------------\n",
      "BLEU            75.17        84.07        78.05        -8.90        -2.88       \n",
      "ROUGE-L         79.73        84.61        81.41        -4.88        -1.68       \n",
      "의미적_유사도(%)      94.29        95.98        95.59        -1.69        -1.30       \n",
      "\n",
      "🎯 파인튜닝 효과 분석\n",
      "================================================================================\n",
      "📈 베이스 모델(GPT-4o-mini) 대비 파인튜닝 개선도:\n",
      "  BLEU: -8.90 (-10.6%)\n",
      "  ROUGE-L: -4.88 (-5.8%)\n",
      "  의미적_유사도(%): -1.69 (-1.8%)\n",
      "\n",
      "🔥 플래그십 모델(GPT-4o) 대비 파인튜닝 성능:\n",
      "  BLEU: -2.88 (열세)\n",
      "  ROUGE-L: -1.68 (열세)\n",
      "  의미적_유사도(%): -1.30 (열세)\n",
      "\n",
      "🏆 최종 결론\n",
      "================================================================================\n",
      "⚠️ 파인튜닝 효과가 제한적입니다.\n",
      "📝 플래그십 모델 대비로는 아직 개선 여지가 있습니다.\n",
      "\n",
      "💰 비용 효율성 분석:\n",
      "  파인튜닝 모델 비용: GPT-4o의 ~10% (훨씬 저렴)\n",
      "  추론 속도: GPT-4o보다 빠름\n",
      "\n",
      "💾 결과가 '3모델_성능비교_결과.json' 파일로 저장되었습니다.\n",
      "\n",
      "✅ 3개 모델 성능 비교 테스트가 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3개 모델 성능 비교: 파인튜닝 vs GPT-4o-mini vs GPT-4o\n",
    "파인튜닝 효과를 명확하게 측정하기 위한 완전한 비교\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ========================================================================================\n",
    "# 전역 설정 및 상수 정의\n",
    "# ========================================================================================\n",
    "\n",
    "# 테스트할 3개 모델 정의\n",
    "FINETUNED_MODEL = \"ft:gpt-4o-mini-2024-07-18:test:pja-erd-finetuning-model:BmOgyrDW:ckpt-step-124\"\n",
    "BASELINE_MINI_MODEL = \"gpt-4o-mini\"  # 파인튜닝 베이스 모델\n",
    "BASELINE_4O_MODEL = \"gpt-4o\"         # 플래그십 모델\n",
    "\n",
    "# API 호출 설정\n",
    "TEMPERATURE = 0.2\n",
    "CSV_FILE_PATH = \"hehe.csv\"\n",
    "\n",
    "# 시스템 프롬프트\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "당신은 ERD 설계 전문가입니다. 프로젝트 요구사항을 분석하여 완전한 데이터베이스 스키마를 생성합니다.\n",
    "**핵심 원칙:**\n",
    "- 백슬래시(\\\\) 절대 사용 금지\n",
    "- erd_relationships의 모든 테이블과 외래키는 반드시 erd_tables에 존재해야 함\n",
    "- 외래키 컬럼은 is_foreign_key: true 설정 필수\n",
    "- 순수 JSON만 응답 (마크다운 블록 금지)\n",
    "**설계 순서:**\n",
    "1. 엔티티 식별 → 2. 속성/기본키 정의 → 3. 관계 분석/외래키 추가 → 4. 관계 정보 작성\n",
    "**중요: 모든 관계의 테이블명과 외래키가 테이블 정의와 정확히 일치해야 합니다.**\n",
    "\"\"\"\n",
    "\n",
    "# 의미적 유사도 계산을 위한 모델\n",
    "try:\n",
    "    semantic_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    SEMANTIC_SIMILARITY_AVAILABLE = True\n",
    "except:\n",
    "    print(\"⚠️ 의미적 유사도 모델을 로드할 수 없습니다. 해당 메트릭은 제외됩니다.\")\n",
    "    SEMANTIC_SIMILARITY_AVAILABLE = False\n",
    "\n",
    "# ========================================================================================\n",
    "# 유틸리티 함수\n",
    "# ========================================================================================\n",
    "\n",
    "def safe_parse_json_string(json_str):\n",
    "    \"\"\"JSON 문자열을 안전하게 파싱하는 함수\"\"\"\n",
    "    if pd.isna(json_str) or json_str == '':\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if isinstance(json_str, (dict, list)):\n",
    "            return json_str\n",
    "        \n",
    "        if isinstance(json_str, str):\n",
    "            try:\n",
    "                return ast.literal_eval(json_str)\n",
    "            except:\n",
    "                return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON 파싱 오류: {e}\")\n",
    "        return json_str\n",
    "\n",
    "def format_data_for_prompt(data):\n",
    "    \"\"\"데이터를 프롬프트에 적합한 형태로 포맷팅\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    elif isinstance(data, (dict, list)):\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        return str(data)\n",
    "\n",
    "def extract_text_content(data):\n",
    "    \"\"\"JSON 데이터에서 텍스트 내용을 추출하여 비교 가능한 형태로 변환\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data.strip()\n",
    "    elif isinstance(data, dict):\n",
    "        text_parts = []\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str):\n",
    "                text_parts.append(f\"{key}: {value}\")\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, str):\n",
    "                        text_parts.append(item)\n",
    "                    elif isinstance(item, dict):\n",
    "                        text_parts.append(json.dumps(item, ensure_ascii=False))\n",
    "        return \" \".join(text_parts)\n",
    "    elif isinstance(data, list):\n",
    "        text_parts = []\n",
    "        for item in data:\n",
    "            if isinstance(item, str):\n",
    "                text_parts.append(item)\n",
    "            elif isinstance(item, dict):\n",
    "                text_parts.append(json.dumps(item, ensure_ascii=False))\n",
    "        return \" \".join(text_parts)\n",
    "    else:\n",
    "        return str(data)\n",
    "\n",
    "# ========================================================================================\n",
    "# 핵심 함수 정의\n",
    "# ========================================================================================\n",
    "\n",
    "def openai_chat_completion(model, user_input, project_info):\n",
    "    \"\"\"OpenAI Chat Completion API를 호출하는 함수\"\"\"\n",
    "    try:\n",
    "        formatted_project_info = format_data_for_prompt(project_info)\n",
    "        formatted_user_input = format_data_for_prompt(user_input)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "프로젝트 정보:\n",
    "{formatted_project_info}\n",
    "\n",
    "요구사항:\n",
    "{formatted_user_input}\n",
    "\n",
    "  **필수 JSON 형식:**\n",
    "  {{\n",
    "    \"erd_tables\": [{{\n",
    "      \"name\": \"테이블명\",\n",
    "      \"erd_columns\": [{{\n",
    "        \"name\": \"컬럼명\",\n",
    "        \"data_type\": \"타입\",\n",
    "        \"is_primary_key\": true/false,\n",
    "        \"is_foreign_key\": true/false,\n",
    "        \"is_nullable\": true/false\n",
    "      }}]\n",
    "    }}],\n",
    "    \"erd_relationships\": [{{\n",
    "      \"from_table\": \"시작테이블\",\n",
    "      \"to_table\": \"끝테이블\",\n",
    "      \"relationship_type\": \"one-to-many\",\n",
    "      \"foreign_key\": \"외래키명\",\n",
    "      \"constraint_name\": \"제약조건명\"\n",
    "    }}]\n",
    "  }}\n",
    "  **핵심 규칙:**\n",
    "  1. 관계의 모든 테이블명이 erd_tables에 존재해야 함\n",
    "  2. 관계의 모든 foreign_key가 해당 테이블 컬럼에 존재해야 함\n",
    "  3. 외래키는 is_foreign_key: true 설정\n",
    "  4. 최소 5개 테이블, 백슬래시 금지, 순수 JSON만\n",
    "  위 규칙을 지켜 완전한 ERD를 생성하세요!\n",
    "\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"API 호출 오류 ({model}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_semantic_similarity(predictions, references):\n",
    "    \"\"\"의미적 유사도를 계산하는 함수\"\"\"\n",
    "    if not SEMANTIC_SIMILARITY_AVAILABLE:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        pred_texts = [extract_text_content(pred) for pred in predictions]\n",
    "        ref_texts = [extract_text_content(ref) for ref in references]\n",
    "        \n",
    "        pred_embeddings = semantic_model.encode(pred_texts)\n",
    "        ref_embeddings = semantic_model.encode(ref_texts)\n",
    "        \n",
    "        similarities = []\n",
    "        for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):\n",
    "            similarity = cosine_similarity([pred_emb], [ref_emb])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        return np.mean(similarities) * 100\n",
    "    except Exception as e:\n",
    "        print(f\"의미적 유사도 계산 오류: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"모델 성능을 평가하는 다양한 메트릭을 계산하는 함수\"\"\"\n",
    "    pred_texts = [extract_text_content(pred) for pred in predictions]\n",
    "    ref_texts = [extract_text_content(ref) for ref in references]\n",
    "    \n",
    "    valid_pairs = [(p, r) for p, r in zip(pred_texts, ref_texts) if p.strip() and r.strip()]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        return {\n",
    "            \"BLEU\": 0.0,\n",
    "            \"ROUGE-L\": 0.0,\n",
    "            \"정확일치율(%)\": 0.0,\n",
    "            \"의미적_유사도(%)\": 0.0\n",
    "        }\n",
    "    \n",
    "    valid_preds, valid_refs = zip(*valid_pairs)\n",
    "    \n",
    "    # BLEU 점수 계산\n",
    "    try:\n",
    "        bleu_score = corpus_bleu(list(valid_preds), [list(valid_refs)]).score\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    \n",
    "    # ROUGE-L 점수 계산\n",
    "    try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        rouge_scores = [scorer.score(ref, pred)['rougeL'].fmeasure \n",
    "                       for ref, pred in zip(valid_refs, valid_preds)]\n",
    "        rouge_l_score = np.mean(rouge_scores) * 100\n",
    "    except:\n",
    "        rouge_l_score = 0.0\n",
    "    \n",
    "    # 정확 일치율 계산\n",
    "    exact_matches = sum(1 for p, r in zip(valid_preds, valid_refs) \n",
    "                       if p.strip() == r.strip())\n",
    "    exact_match_rate = (exact_matches / len(valid_pairs)) * 100\n",
    "    \n",
    "    # 의미적 유사도 계산\n",
    "    semantic_similarity = calculate_semantic_similarity(predictions, references)\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": round(bleu_score, 2),\n",
    "        \"ROUGE-L\": round(rouge_l_score, 2),\n",
    "        \"정확일치율(%)\": round(exact_match_rate, 2),\n",
    "        \"의미적_유사도(%)\": round(semantic_similarity, 2)\n",
    "    }\n",
    "\n",
    "def run_three_model_evaluation():\n",
    "    \"\"\"3개 모델 비교 평가 실행 함수\"\"\"\n",
    "    print(\"📊 3개 모델 성능 비교: 파인튜닝 vs GPT-4o-mini vs GPT-4o\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 1. 데이터 로드 및 검증\n",
    "    # ========================================================================================\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_FILE_PATH)\n",
    "        print(f\"✅ 데이터 로드 완료: {len(df)}개 샘플\")\n",
    "        print(f\"📋 CSV 컬럼: {list(df.columns)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CSV 파일 로드 실패: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 데이터 전처리\n",
    "    print(\"\\n🔄 데이터 전처리 중...\")\n",
    "    for col in ['user_input', 'project_info', 'total_requirements', 'ERD_data']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(safe_parse_json_string)\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 2. 3개 모델 추론 실행\n",
    "    # ========================================================================================\n",
    "    \n",
    "    # 테스트할 샘플 수 제한\n",
    "    test_samples = min(5, len(df))\n",
    "    test_df = df.head(test_samples).copy()\n",
    "    \n",
    "    print(f\"\\n🔄 {test_samples}개 샘플에 대해 3개 모델 추론 시작...\")\n",
    "    \n",
    "    # 3개 모델의 결과를 저장할 딕셔너리\n",
    "    results = {\n",
    "        FINETUNED_MODEL: [],\n",
    "        BASELINE_MINI_MODEL: [],\n",
    "        BASELINE_4O_MODEL: []\n",
    "    }\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"모델 추론 진행\"):\n",
    "        user_input = row['user_input']\n",
    "        project_info = row['project_info']\n",
    "        \n",
    "        print(f\"\\n📝 샘플 {idx+1} 처리 중...\")\n",
    "        \n",
    "        # 1. 파인튜닝 모델 추론\n",
    "        print(f\"   🎯 파인튜닝 모델 추론...\")\n",
    "        ft_result = openai_chat_completion(FINETUNED_MODEL, user_input, project_info)\n",
    "        results[FINETUNED_MODEL].append(ft_result)\n",
    "        \n",
    "        # 2. GPT-4o-mini 원본 추론\n",
    "        print(f\"   🤖 GPT-4o-mini 추론...\")\n",
    "        mini_result = openai_chat_completion(BASELINE_MINI_MODEL, user_input, project_info)\n",
    "        results[BASELINE_MINI_MODEL].append(mini_result)\n",
    "        \n",
    "        # 3. GPT-4o 추론\n",
    "        print(f\"   🔥 GPT-4o 추론...\")\n",
    "        gpt4o_result = openai_chat_completion(BASELINE_4O_MODEL, user_input, project_info)\n",
    "        results[BASELINE_4O_MODEL].append(gpt4o_result)\n",
    "        \n",
    "        time.sleep(1.0)  # API 레이트 리미트 방지\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 3. 성능 메트릭 계산 및 비교\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(\"\\n📈 3개 모델 성능 평가 결과\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 정답 데이터\n",
    "    references = test_df['ERD_data'].tolist()\n",
    "    \n",
    "    # 모델별 성능 계산\n",
    "    final_results = {}\n",
    "    model_names = {\n",
    "        FINETUNED_MODEL: \"파인튜닝_모델\",\n",
    "        BASELINE_MINI_MODEL: \"GPT-4o-mini\", \n",
    "        BASELINE_4O_MODEL: \"GPT-4o\"\n",
    "    }\n",
    "    \n",
    "    for model_id, predictions in results.items():\n",
    "        model_name = model_names[model_id]\n",
    "        \n",
    "        print(f\"\\n🔍 {model_name} 출력 확인:\")\n",
    "        for i, pred in enumerate(predictions[:2]):\n",
    "            print(f\"  출력 {i+1}: {str(pred)[:150]}...\")\n",
    "        \n",
    "        metrics = calculate_metrics(predictions, references)\n",
    "        \n",
    "        print(f\"\\n🤖 {model_name} 성능:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"   {metric_name}: {value}\")\n",
    "        \n",
    "        final_results[model_name] = metrics\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 4. 종합 비교 분석\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n📊 종합 성능 비교 분석\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 성능 비교 표 출력\n",
    "    metrics_list = [\"BLEU\", \"ROUGE-L\", \"의미적_유사도(%)\"]\n",
    "    \n",
    "    print(f\"{'메트릭':<15} {'파인튜닝':<12} {'4o-mini':<12} {'GPT-4o':<12} {'vs mini':<12} {'vs 4o':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in metrics_list:\n",
    "        ft_score = final_results[\"파인튜닝_모델\"][metric]\n",
    "        mini_score = final_results[\"GPT-4o-mini\"][metric]\n",
    "        gpt4o_score = final_results[\"GPT-4o\"][metric]\n",
    "        \n",
    "        vs_mini = ft_score - mini_score\n",
    "        vs_4o = ft_score - gpt4o_score\n",
    "        \n",
    "        print(f\"{metric:<15} {ft_score:<12.2f} {mini_score:<12.2f} {gpt4o_score:<12.2f} {vs_mini:<+12.2f} {vs_4o:<+12.2f}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 5. 파인튜닝 효과 분석\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n🎯 파인튜닝 효과 분석\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ft_metrics = final_results[\"파인튜닝_모델\"]\n",
    "    mini_metrics = final_results[\"GPT-4o-mini\"]\n",
    "    gpt4o_metrics = final_results[\"GPT-4o\"]\n",
    "    \n",
    "    # 베이스 모델(4o-mini) 대비 개선도\n",
    "    print(\"📈 베이스 모델(GPT-4o-mini) 대비 파인튜닝 개선도:\")\n",
    "    mini_improvements = 0\n",
    "    for metric in metrics_list:\n",
    "        diff = ft_metrics[metric] - mini_metrics[metric]\n",
    "        improvement_rate = (diff / mini_metrics[metric]) * 100 if mini_metrics[metric] != 0 else 0\n",
    "        print(f\"  {metric}: {diff:+.2f} ({improvement_rate:+.1f}%)\")\n",
    "        if diff > 0:\n",
    "            mini_improvements += 1\n",
    "    \n",
    "    # 플래그십 모델(GPT-4o) 대비 성능\n",
    "    print(f\"\\n🔥 플래그십 모델(GPT-4o) 대비 파인튜닝 성능:\")\n",
    "    gpt4o_wins = 0\n",
    "    for metric in metrics_list:\n",
    "        diff = ft_metrics[metric] - gpt4o_metrics[metric]\n",
    "        print(f\"  {metric}: {diff:+.2f} ({'우수' if diff > 0 else '열세'})\")\n",
    "        if diff > 0:\n",
    "            gpt4o_wins += 1\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 6. 결론 및 권장사항\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n🏆 최종 결론\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if mini_improvements >= 2:\n",
    "        print(\"✅ 파인튜닝이 베이스 모델 대비 명확한 성능 향상을 보여줍니다!\")\n",
    "    else:\n",
    "        print(\"⚠️ 파인튜닝 효과가 제한적입니다.\")\n",
    "    \n",
    "    if gpt4o_wins >= 2:\n",
    "        print(\"🔥 파인튜닝 모델이 플래그십 모델도 능가하는 놀라운 성과입니다!\")\n",
    "    elif gpt4o_wins >= 1:\n",
    "        print(\"👍 파인튜닝 모델이 일부 지표에서 플래그십 모델과 경쟁합니다!\")\n",
    "    else:\n",
    "        print(\"📝 플래그십 모델 대비로는 아직 개선 여지가 있습니다.\")\n",
    "    \n",
    "    # 비용 효율성 분석\n",
    "    print(f\"\\n💰 비용 효율성 분석:\")\n",
    "    print(f\"  파인튜닝 모델 비용: GPT-4o의 ~10% (훨씬 저렴)\")\n",
    "    print(f\"  추론 속도: GPT-4o보다 빠름\")\n",
    "    if gpt4o_wins >= 1:\n",
    "        print(f\"  성능: 일부 지표에서 GPT-4o 수준 또는 그 이상\")\n",
    "        print(f\"  → 🎯 매우 높은 ROI!\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 7. 결과 저장\n",
    "    # ========================================================================================\n",
    "    \n",
    "    try:\n",
    "        with open(\"3모델_성능비교_결과.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n💾 결과가 '3모델_성능비교_결과.json' 파일로 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 결과 저장 실패: {e}\")\n",
    "\n",
    "# ========================================================================================\n",
    "# 메인 실행 부분\n",
    "# ========================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"❌ OPENAI_API_KEY 환경변수를 설정해주세요.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"🚀 3개 모델 성능 비교 테스트를 시작합니다...\")\n",
    "    print(f\"📁 테스트 데이터: {CSV_FILE_PATH}\")\n",
    "    print(f\"🎯 파인튜닝 모델: {FINETUNED_MODEL}\")\n",
    "    print(f\"🤖 GPT-4o-mini: {BASELINE_MINI_MODEL}\")\n",
    "    print(f\"🔥 GPT-4o: {BASELINE_4O_MODEL}\")\n",
    "    print(f\"🌡️ 온도 설정: {TEMPERATURE}\")\n",
    "    \n",
    "    run_three_model_evaluation()\n",
    "    \n",
    "    print(\"\\n✅ 3개 모델 성능 비교 테스트가 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64a98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"모델_출력_비교_개선.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d821bac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>total_requirements</th>\n",
       "      <th>project_info</th>\n",
       "      <th>ERD_data</th>\n",
       "      <th>파인튜닝_모델_출력</th>\n",
       "      <th>GPT4o_출력</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'projectName': '스터디 그룹 성과 분석 도구', 'projectTa...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '스터디 그룹 성과 분석 도구', ...</td>\n",
       "      <td>{'erd_tables': [{'name': 'users', 'erd_columns...</td>\n",
       "      <td>아래는 \"스터디 그룹 성과 분석 도구\" 프로젝트에 대한 ERD(Entity Rela...</td>\n",
       "      <td>아래는 \"스터디 그룹 성과 분석 도구\" 프로젝트를 위한 ERD 데이터입니다. 이 데...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'projectName': '기억의 다리', 'projectTarget': '치...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '기억의 다리', 'category...</td>\n",
       "      <td>{'erd_tables': [{'name': 'Users', 'erd_columns...</td>\n",
       "      <td>아래는 \"기억의 다리\" 프로젝트를 위한 ERD(Entity Relationship ...</td>\n",
       "      <td>```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'projectName': '장애인 친화 대중교통 안내 앱', 'projectT...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '장애인 친화 대중교통 안내 앱',...</td>\n",
       "      <td>{'erd_tables': [{'name': 'Users', 'erd_columns...</td>\n",
       "      <td>아래는 \"장애인 친화 대중교통 안내 앱\"의 ERD(Entity Relationshi...</td>\n",
       "      <td>```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'projectName': '가상 피팅룸 서비스', 'projectTarget'...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '가상 피팅룸 서비스', 'cate...</td>\n",
       "      <td>{'erd_tables': [{'name': '사용자', 'erd_columns':...</td>\n",
       "      <td>아래는 \"가상 피팅룸 서비스\"를 위한 ERD(Entity Relationship D...</td>\n",
       "      <td>```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'projectName': '스마트 건강 관리 비서', 'projectTarge...</td>\n",
       "      <td>[{'requirementType': 'FUNCTIONAL', 'content': ...</td>\n",
       "      <td>{'project_info': {'title': '스마트 건강 관리 비서', 'ca...</td>\n",
       "      <td>{'erd_tables': [{'name': 'Users', 'erd_columns...</td>\n",
       "      <td>아래는 \"스마트 건강 관리 비서\" 프로젝트를 위한 ERD(Entity Relatio...</td>\n",
       "      <td>```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  [{'projectName': '스터디 그룹 성과 분석 도구', 'projectTa...   \n",
       "1  [{'projectName': '기억의 다리', 'projectTarget': '치...   \n",
       "2  [{'projectName': '장애인 친화 대중교통 안내 앱', 'projectT...   \n",
       "3  [{'projectName': '가상 피팅룸 서비스', 'projectTarget'...   \n",
       "4  [{'projectName': '스마트 건강 관리 비서', 'projectTarge...   \n",
       "\n",
       "                                  total_requirements  \\\n",
       "0  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "1  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "2  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "3  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "4  [{'requirementType': 'FUNCTIONAL', 'content': ...   \n",
       "\n",
       "                                        project_info  \\\n",
       "0  {'project_info': {'title': '스터디 그룹 성과 분석 도구', ...   \n",
       "1  {'project_info': {'title': '기억의 다리', 'category...   \n",
       "2  {'project_info': {'title': '장애인 친화 대중교통 안내 앱',...   \n",
       "3  {'project_info': {'title': '가상 피팅룸 서비스', 'cate...   \n",
       "4  {'project_info': {'title': '스마트 건강 관리 비서', 'ca...   \n",
       "\n",
       "                                            ERD_data  \\\n",
       "0  {'erd_tables': [{'name': 'users', 'erd_columns...   \n",
       "1  {'erd_tables': [{'name': 'Users', 'erd_columns...   \n",
       "2  {'erd_tables': [{'name': 'Users', 'erd_columns...   \n",
       "3  {'erd_tables': [{'name': '사용자', 'erd_columns':...   \n",
       "4  {'erd_tables': [{'name': 'Users', 'erd_columns...   \n",
       "\n",
       "                                          파인튜닝_모델_출력  \\\n",
       "0  아래는 \"스터디 그룹 성과 분석 도구\" 프로젝트에 대한 ERD(Entity Rela...   \n",
       "1  아래는 \"기억의 다리\" 프로젝트를 위한 ERD(Entity Relationship ...   \n",
       "2  아래는 \"장애인 친화 대중교통 안내 앱\"의 ERD(Entity Relationshi...   \n",
       "3  아래는 \"가상 피팅룸 서비스\"를 위한 ERD(Entity Relationship D...   \n",
       "4  아래는 \"스마트 건강 관리 비서\" 프로젝트를 위한 ERD(Entity Relatio...   \n",
       "\n",
       "                                            GPT4o_출력  \n",
       "0  아래는 \"스터디 그룹 성과 분석 도구\" 프로젝트를 위한 ERD 데이터입니다. 이 데...  \n",
       "1  ```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...  \n",
       "2  ```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...  \n",
       "3  ```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...  \n",
       "4  ```json\\n{\\n  \"entities\": [\\n    {\\n      \"nam...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30517074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ERD 특화 3모델 성능 비교를 시작합니다...\n",
      "📁 테스트 데이터: hehe.csv\n",
      "🎯 파인튜닝 모델: ft:gpt-4o-mini-2024-07-18:test:pja-api-finetuning-model:BmOdcDUE\n",
      "🤖 GPT-4o-mini: gpt-4o-mini\n",
      "🔥 GPT-4o: gpt-4o\n",
      "🌡️ 온도 설정: 0.2\n",
      "📊 평가 방식: ERD 구조적 품질 및 정확성 중심\n",
      "🎯 ERD 특화 3모델 성능 비교: 구조적 품질과 정확성 중심 평가\n",
      "================================================================================\n",
      "✅ 데이터 로드 완료: 138개 샘플\n",
      "📋 CSV 컬럼: ['user_input', 'total_requirements', 'project_info', 'ERD_data']\n",
      "\n",
      "🔄 데이터 전처리 중...\n",
      "\n",
      "🔄 5개 샘플에 대해 3개 모델 ERD 생성 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 1 처리 중...\n",
      "   🎯 파인튜닝_모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  20%|██        | 1/5 [00:51<03:25, 51.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 2 처리 중...\n",
      "   🎯 파인튜닝_모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  40%|████      | 2/5 [01:48<02:44, 54.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 3 처리 중...\n",
      "   🎯 파인튜닝_모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  60%|██████    | 3/5 [02:37<01:44, 52.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 4 처리 중...\n",
      "   🎯 파인튜닝_모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행:  80%|████████  | 4/5 [03:33<00:53, 53.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 5 처리 중...\n",
      "   🎯 파인튜닝_모델 추론...\n",
      "   🤖 GPT-4o-mini 추론...\n",
      "   🔥 GPT-4o 추론...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 추론 진행: 100%|██████████| 5/5 [04:27<00:00, 53.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 ERD 특화 평가 결과 분석\n",
      "================================================================================\n",
      "\n",
      "🔍 파인튜닝_모델 분석 중...\n",
      "   JSON 파싱 성공률: 100.0%\n",
      "   평균 테이블 수: 5.6\n",
      "   설계 품질 점수: 93.4%\n",
      "   전체 구조 일치도: 12.5%\n",
      "\n",
      "🔍 GPT-4o-mini 분석 중...\n",
      "   JSON 파싱 성공률: 100.0%\n",
      "   평균 테이블 수: 5.0\n",
      "   설계 품질 점수: 99.7%\n",
      "   전체 구조 일치도: 13.2%\n",
      "\n",
      "🔍 GPT-4o 분석 중...\n",
      "   JSON 파싱 성공률: 100.0%\n",
      "   평균 테이블 수: 5.0\n",
      "   설계 품질 점수: 100.0%\n",
      "   전체 구조 일치도: 16.2%\n",
      "\n",
      "📈 ERD 특화 성능 비교 분석\n",
      "====================================================================================================\n",
      "메트릭                  파인튜닝         4o-mini      GPT-4o       vs mini      vs 4o       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "JSON_파싱_성공률(%)       100.0        100.0        100.0        +0.0         +0.0        \n",
      "설계품질_점수(%)           93.4         99.7         100.0        -6.3         -6.6        \n",
      "전체구조_일치도(%)          12.5         13.2         16.2         -0.7         -3.7        \n",
      "관계_일관성(%)            100.0        95.0         100.0        +5.0         +0.0        \n",
      "완성도_점수(%)            100.0        100.0        100.0        +0.0         +0.0        \n",
      "\n",
      "🔍 세부 ERD 품질 분석\n",
      "================================================================================\n",
      "\n",
      "📊 구조적 품질 분석:\n",
      "  평균_테이블_수: 파인튜닝=5.6, 4o-mini=5.0, GPT-4o=5.0\n",
      "  평균_관계_수: 파인튜닝=5.0, 4o-mini=4.4, GPT-4o=4.2\n",
      "  평균_컬럼_수: 파인튜닝=17.6, 4o-mini=18.6, GPT-4o=21.6\n",
      "\n",
      "🔗 스키마 일관성 분석:\n",
      "  외래키_일관성(%): 파인튜닝=60.0%, 4o-mini=95.0%, GPT-4o=95.0%\n",
      "  관계_일관성(%): 파인튜닝=100.0%, 4o-mini=95.0%, GPT-4o=100.0%\n",
      "  네이밍_일관성(%): 파인튜닝=20.0%, 4o-mini=96.0%, GPT-4o=96.4%\n",
      "\n",
      "🎯 구조 정확성 분석:\n",
      "  테이블명_일치도(%): 파인튜닝=16.0%, 4o-mini=16.0%, GPT-4o=24.0%\n",
      "  컬럼구조_일치도(%): 파인튜닝=11.2%, 4o-mini=12.1%, GPT-4o=16.4%\n",
      "  관계구조_일치도(%): 파인튜닝=8.0%, 4o-mini=10.0%, GPT-4o=0.0%\n",
      "\n",
      "🎯 파인튜닝 효과 심층 분석\n",
      "================================================================================\n",
      "📈 베이스 모델(GPT-4o-mini) 대비 파인튜닝 개선도:\n",
      "  JSON_파싱_성공률(%): +0.0 (+0.0%)\n",
      "  설계품질_점수(%): -6.3 (-6.3%)\n",
      "  전체구조_일치도(%): -0.7 (-5.5%)\n",
      "  관계_일관성(%): +5.0 (+5.3%)\n",
      "  완성도_점수(%): +0.0 (+0.0%)\n",
      "\n",
      "🔥 플래그십 모델(GPT-4o) 대비 파인튜닝 성능:\n",
      "  JSON_파싱_성공률(%): +0.0 (🟡 동등)\n",
      "  설계품질_점수(%): -6.6 (🔴 열세)\n",
      "  전체구조_일치도(%): -3.7 (🔴 열세)\n",
      "  관계_일관성(%): +0.0 (🟡 동등)\n",
      "  완성도_점수(%): +0.0 (🟡 동등)\n",
      "\n",
      "⚡ 모델별 강점 분석\n",
      "================================================================================\n",
      "\n",
      "🏆 파인튜닝_모델 최우수 영역:\n",
      "  • JSON_파싱_성공률(%): 100.0\n",
      "  • 평균_테이블_수: 5.6\n",
      "  • 평균_관계_수: 5.0\n",
      "  • 관계_일관성(%): 100.0\n",
      "  • 완성도_점수(%): 100.0\n",
      "\n",
      "🏆 GPT-4o-mini 최우수 영역:\n",
      "  • 외래키_일관성(%): 95.0\n",
      "  • 정규화_점수(%): 100.0\n",
      "  • 관계구조_일치도(%): 10.0\n",
      "\n",
      "🏆 GPT-4o 최우수 영역:\n",
      "  • 평균_컬럼_수: 21.6\n",
      "  • 네이밍_일관성(%): 96.4\n",
      "  • 설계품질_점수(%): 100.0\n",
      "  • 테이블명_일치도(%): 24.0\n",
      "  • 컬럼구조_일치도(%): 16.4\n",
      "\n",
      "💼 실용성 평가\n",
      "================================================================================\n",
      "🔧 JSON 생성 안정성:\n",
      "  파인튜닝: 100.0% (🟢 매우안정)\n",
      "  4o-mini: 100.0% (🟢 매우안정)\n",
      "  GPT-4o: 100.0% (🟢 매우안정)\n",
      "\n",
      "🎨 ERD 설계 품질:\n",
      "  파인튜닝: 93.4% (🟢 우수)\n",
      "  4o-mini: 99.7% (🟢 우수)\n",
      "  GPT-4o: 100.0% (🟢 우수)\n",
      "\n",
      "🏆 최종 결론 및 권장사항\n",
      "================================================================================\n",
      "📊 파인튜닝 효과: 🔴 파인튜닝 효과 제한적. 추가 최적화 필요\n",
      "   • 1/5 주요 메트릭에서 개선\n",
      "   • 평균 개선율: -1.3%\n",
      "\n",
      "🎯 GPT-4o 대비: 📈 플래그십 모델 대비 개선 여지 존재\n",
      "   • 0/5 주요 메트릭에서 우세\n",
      "\n",
      "💡 실용적 권장사항:\n",
      "  🔥 GPT-4o 사용 권장:\n",
      "    - 최고 품질의 ERD 설계\n",
      "    - 복잡한 요구사항 처리 우수\n",
      "    - 높은 일관성\n",
      "\n",
      "💰 비용 대비 성능 분석:\n",
      "  파인튜닝 모델: 성능 81.2, 비용 ⭐\n",
      "  GPT-4o-mini: 성능 81.6, 비용 ⭐⭐\n",
      "  GPT-4o: 성능 83.2, 비용 ⭐⭐⭐⭐⭐\n",
      "  💎 최고 ROI: 파인튜닝 (비용효율점수: 405.9)\n",
      "\n",
      "💾 상세 결과가 'ERD특화_3모델_성능비교_결과.json' 파일로 저장되었습니다.\n",
      "📄 요약 리포트가 'ERD특화_성능비교_요약.txt' 파일로 저장되었습니다.\n",
      "\n",
      "✅ ERD 특화 3모델 성능 비교가 완료되었습니다!\n",
      "📋 결과 파일:\n",
      "   • ERD특화_3모델_성능비교_결과.json (상세 결과)\n",
      "   • ERD특화_성능비교_요약.txt (요약 리포트)\n",
      "\n",
      "🎯 이제 ERD 설계에 최적화된 평가로 정확한 모델 성능을 확인할 수 있습니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ERD 특화 3모델 성능 비교: 구조적 품질과 정확성 중심 평가\n",
    "JSON 구조, 스키마 정확성, ERD 설계 품질을 직접 측정하는 전문 평가 시스템\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# ========================================================================================\n",
    "# 전역 설정 및 상수 정의\n",
    "# ========================================================================================\n",
    "\n",
    "# 테스트할 3개 모델 정의\n",
    "FINETUNED_MODEL = \"ft:gpt-4o-mini-2024-07-18:test:pja-api-finetuning-model:BmOdcDUE\"\n",
    "BASELINE_MINI_MODEL = \"gpt-4o-mini\"\n",
    "BASELINE_4O_MODEL = \"gpt-4o\"\n",
    "\n",
    "# API 호출 설정\n",
    "TEMPERATURE = 0.2\n",
    "CSV_FILE_PATH = \"hehe.csv\"\n",
    "\n",
    "# 시스템 프롬프트\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "당신은 ERD 설계 전문가입니다. 프로젝트 요구사항을 분석하여 완전한 데이터베이스 스키마를 생성합니다.\n",
    "**핵심 원칙:**\n",
    "- 백슬래시(\\\\) 절대 사용 금지\n",
    "- erd_relationships의 모든 테이블과 외래키는 반드시 erd_tables에 존재해야 함\n",
    "- 외래키 컬럼은 is_foreign_key: true 설정 필수\n",
    "- 순수 JSON만 응답 (마크다운 블록 금지)\n",
    "**설계 순서:**\n",
    "1. 엔티티 식별 → 2. 속성/기본키 정의 → 3. 관계 분석/외래키 추가 → 4. 관계 정보 작성\n",
    "**중요: 모든 관계의 테이블명과 외래키가 테이블 정의와 정확히 일치해야 합니다.**\n",
    "\"\"\"\n",
    "\n",
    "# ========================================================================================\n",
    "# ERD 특화 평가 함수들\n",
    "# ========================================================================================\n",
    "\n",
    "def safe_parse_json(json_str: str) -> Tuple[Dict, bool]:\n",
    "    \"\"\"JSON 파싱을 시도하고 성공 여부를 반환\"\"\"\n",
    "    if pd.isna(json_str) or json_str == '':\n",
    "        return {}, False\n",
    "    \n",
    "    try:\n",
    "        if isinstance(json_str, (dict, list)):\n",
    "            return json_str, True\n",
    "        \n",
    "        if isinstance(json_str, str):\n",
    "            # 마크다운 코드 블록 제거\n",
    "            json_str = re.sub(r'```json\\s*|\\s*```', '', json_str.strip())\n",
    "            json_str = json_str.strip()\n",
    "            \n",
    "            try:\n",
    "                return json.loads(json_str), True\n",
    "            except:\n",
    "                return ast.literal_eval(json_str), True\n",
    "    except Exception as e:\n",
    "        return {}, False\n",
    "\n",
    "def validate_erd_structure(erd_data: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"ERD JSON 구조의 유효성을 검사\"\"\"\n",
    "    validation_result = {\n",
    "        'is_valid_json': True,\n",
    "        'has_erd_tables': False,\n",
    "        'has_erd_relationships': False,\n",
    "        'table_count': 0,\n",
    "        'relationship_count': 0,\n",
    "        'column_count': 0,\n",
    "        'foreign_key_count': 0,\n",
    "        'primary_key_count': 0,\n",
    "        'structural_issues': []\n",
    "    }\n",
    "    \n",
    "    if not isinstance(erd_data, dict):\n",
    "        validation_result['is_valid_json'] = False\n",
    "        validation_result['structural_issues'].append(\"Invalid JSON structure\")\n",
    "        return validation_result\n",
    "    \n",
    "    # 필수 키 존재 확인\n",
    "    if 'erd_tables' in erd_data:\n",
    "        validation_result['has_erd_tables'] = True\n",
    "        tables = erd_data['erd_tables']\n",
    "        if isinstance(tables, list):\n",
    "            validation_result['table_count'] = len(tables)\n",
    "            \n",
    "            # 테이블별 컬럼 분석\n",
    "            for table in tables:\n",
    "                if isinstance(table, dict) and 'erd_columns' in table:\n",
    "                    columns = table['erd_columns']\n",
    "                    if isinstance(columns, list):\n",
    "                        validation_result['column_count'] += len(columns)\n",
    "                        \n",
    "                        for col in columns:\n",
    "                            if isinstance(col, dict):\n",
    "                                if col.get('is_foreign_key', False):\n",
    "                                    validation_result['foreign_key_count'] += 1\n",
    "                                if col.get('is_primary_key', False):\n",
    "                                    validation_result['primary_key_count'] += 1\n",
    "        else:\n",
    "            validation_result['structural_issues'].append(\"erd_tables is not a list\")\n",
    "    else:\n",
    "        validation_result['structural_issues'].append(\"Missing erd_tables\")\n",
    "    \n",
    "    if 'erd_relationships' in erd_data:\n",
    "        validation_result['has_erd_relationships'] = True\n",
    "        relationships = erd_data['erd_relationships']\n",
    "        if isinstance(relationships, list):\n",
    "            validation_result['relationship_count'] = len(relationships)\n",
    "        else:\n",
    "            validation_result['structural_issues'].append(\"erd_relationships is not a list\")\n",
    "    else:\n",
    "        validation_result['structural_issues'].append(\"Missing erd_relationships\")\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "def calculate_schema_consistency(erd_data: Dict) -> Dict[str, float]:\n",
    "    \"\"\"ERD 스키마의 일관성을 계산\"\"\"\n",
    "    consistency_metrics = {\n",
    "        'foreign_key_consistency': 0.0,  # 외래키가 실제 테이블에 존재하는가\n",
    "        'relationship_consistency': 0.0,  # 관계의 테이블들이 실제 존재하는가\n",
    "        'data_type_consistency': 0.0,    # 데이터 타입이 일관되게 정의되었는가\n",
    "        'naming_consistency': 0.0        # 네이밍 규칙이 일관되는가\n",
    "    }\n",
    "    \n",
    "    if not isinstance(erd_data, dict):\n",
    "        return consistency_metrics\n",
    "    \n",
    "    tables = erd_data.get('erd_tables', [])\n",
    "    relationships = erd_data.get('erd_relationships', [])\n",
    "    \n",
    "    if not tables:\n",
    "        return consistency_metrics\n",
    "    \n",
    "    # 테이블명과 컬럼 정보 수집\n",
    "    table_names = set()\n",
    "    table_columns = {}\n",
    "    all_columns = []\n",
    "    \n",
    "    for table in tables:\n",
    "        if isinstance(table, dict) and 'name' in table:\n",
    "            table_name = table['name']\n",
    "            table_names.add(table_name)\n",
    "            \n",
    "            columns = table.get('erd_columns', [])\n",
    "            table_columns[table_name] = {}\n",
    "            \n",
    "            for col in columns:\n",
    "                if isinstance(col, dict) and 'name' in col:\n",
    "                    col_name = col['name']\n",
    "                    table_columns[table_name][col_name] = col\n",
    "                    all_columns.append(col)\n",
    "    \n",
    "    # 1. 외래키 일관성 검사\n",
    "    if all_columns:\n",
    "        foreign_keys = [col for col in all_columns if col.get('is_foreign_key', False)]\n",
    "        if foreign_keys:\n",
    "            valid_fk_count = 0\n",
    "            for fk in foreign_keys:\n",
    "                # 외래키가 참조하는 테이블이 존재하는지 간접적으로 확인\n",
    "                # (실제 참조 테이블 정보가 없으므로 외래키 명명 규칙으로 추정)\n",
    "                fk_name = fk.get('name', '')\n",
    "                if '_id' in fk_name or 'id' in fk_name:\n",
    "                    valid_fk_count += 1\n",
    "            consistency_metrics['foreign_key_consistency'] = valid_fk_count / len(foreign_keys) * 100\n",
    "        else:\n",
    "            consistency_metrics['foreign_key_consistency'] = 100.0  # 외래키가 없으면 만점\n",
    "    \n",
    "    # 2. 관계 일관성 검사\n",
    "    if relationships and table_names:\n",
    "        valid_relationship_count = 0\n",
    "        for rel in relationships:\n",
    "            if isinstance(rel, dict):\n",
    "                from_table = rel.get('from_table', '')\n",
    "                to_table = rel.get('to_table', '')\n",
    "                foreign_key = rel.get('foreign_key', '')\n",
    "                \n",
    "                # 테이블 존재 여부 확인\n",
    "                tables_exist = from_table in table_names and to_table in table_names\n",
    "                \n",
    "                # 외래키가 실제 컬럼에 존재하는지 확인\n",
    "                fk_exists = False\n",
    "                if from_table in table_columns:\n",
    "                    fk_exists = foreign_key in table_columns[from_table]\n",
    "                \n",
    "                if tables_exist and fk_exists:\n",
    "                    valid_relationship_count += 1\n",
    "        \n",
    "        consistency_metrics['relationship_consistency'] = valid_relationship_count / len(relationships) * 100\n",
    "    else:\n",
    "        consistency_metrics['relationship_consistency'] = 100.0 if not relationships else 0.0\n",
    "    \n",
    "    # 3. 데이터 타입 일관성 (공통 타입들이 일관되게 사용되는가)\n",
    "    if all_columns:\n",
    "        data_types = [col.get('data_type', '').lower() for col in all_columns if col.get('data_type')]\n",
    "        if data_types:\n",
    "            # 표준 데이터 타입과의 일치도 측정\n",
    "            standard_types = {'int', 'varchar', 'text', 'date', 'datetime', 'boolean', 'decimal', 'float'}\n",
    "            standard_type_count = sum(1 for dt in data_types if any(st in dt for st in standard_types))\n",
    "            consistency_metrics['data_type_consistency'] = standard_type_count / len(data_types) * 100\n",
    "        else:\n",
    "            consistency_metrics['data_type_consistency'] = 0.0\n",
    "    \n",
    "    # 4. 네이밍 일관성 (snake_case 등 일관된 명명 규칙)\n",
    "    all_names = list(table_names)\n",
    "    for cols in table_columns.values():\n",
    "        all_names.extend(cols.keys())\n",
    "    \n",
    "    if all_names:\n",
    "        snake_case_count = sum(1 for name in all_names if re.match(r'^[a-z][a-z0-9_]*$', name))\n",
    "        consistency_metrics['naming_consistency'] = snake_case_count / len(all_names) * 100\n",
    "    \n",
    "    return consistency_metrics\n",
    "\n",
    "def compare_erd_structures(pred_erd: Dict, ref_erd: Dict) -> Dict[str, float]:\n",
    "    \"\"\"두 ERD 구조를 비교하여 유사도를 계산\"\"\"\n",
    "    comparison_metrics = {\n",
    "        'table_name_similarity': 0.0,      # 테이블명 일치도\n",
    "        'column_structure_similarity': 0.0, # 컬럼 구조 유사도\n",
    "        'relationship_similarity': 0.0,     # 관계 구조 유사도\n",
    "        'overall_structure_similarity': 0.0 # 전체 구조 유사도\n",
    "    }\n",
    "    \n",
    "    # 테이블명 비교\n",
    "    pred_tables = {table.get('name', '') for table in pred_erd.get('erd_tables', []) if isinstance(table, dict)}\n",
    "    ref_tables = {table.get('name', '') for table in ref_erd.get('erd_tables', []) if isinstance(table, dict)}\n",
    "    \n",
    "    if ref_tables:\n",
    "        table_intersection = pred_tables.intersection(ref_tables)\n",
    "        comparison_metrics['table_name_similarity'] = len(table_intersection) / len(ref_tables) * 100\n",
    "    \n",
    "    # 컬럼 구조 비교\n",
    "    pred_columns = set()\n",
    "    ref_columns = set()\n",
    "    \n",
    "    for table in pred_erd.get('erd_tables', []):\n",
    "        if isinstance(table, dict):\n",
    "            table_name = table.get('name', '')\n",
    "            for col in table.get('erd_columns', []):\n",
    "                if isinstance(col, dict):\n",
    "                    pred_columns.add(f\"{table_name}.{col.get('name', '')}\")\n",
    "    \n",
    "    for table in ref_erd.get('erd_tables', []):\n",
    "        if isinstance(table, dict):\n",
    "            table_name = table.get('name', '')\n",
    "            for col in table.get('erd_columns', []):\n",
    "                if isinstance(col, dict):\n",
    "                    ref_columns.add(f\"{table_name}.{col.get('name', '')}\")\n",
    "    \n",
    "    if ref_columns:\n",
    "        column_intersection = pred_columns.intersection(ref_columns)\n",
    "        comparison_metrics['column_structure_similarity'] = len(column_intersection) / len(ref_columns) * 100\n",
    "    \n",
    "    # 관계 비교\n",
    "    pred_relationships = set()\n",
    "    ref_relationships = set()\n",
    "    \n",
    "    for rel in pred_erd.get('erd_relationships', []):\n",
    "        if isinstance(rel, dict):\n",
    "            rel_key = f\"{rel.get('from_table', '')}->{rel.get('to_table', '')}\"\n",
    "            pred_relationships.add(rel_key)\n",
    "    \n",
    "    for rel in ref_erd.get('erd_relationships', []):\n",
    "        if isinstance(rel, dict):\n",
    "            rel_key = f\"{rel.get('from_table', '')}->{rel.get('to_table', '')}\"\n",
    "            ref_relationships.add(rel_key)\n",
    "    \n",
    "    if ref_relationships:\n",
    "        rel_intersection = pred_relationships.intersection(ref_relationships)\n",
    "        comparison_metrics['relationship_similarity'] = len(rel_intersection) / len(ref_relationships) * 100\n",
    "    \n",
    "    # 전체 구조 유사도 (가중 평균)\n",
    "    comparison_metrics['overall_structure_similarity'] = (\n",
    "        comparison_metrics['table_name_similarity'] * 0.4 +\n",
    "        comparison_metrics['column_structure_similarity'] * 0.4 +\n",
    "        comparison_metrics['relationship_similarity'] * 0.2\n",
    "    )\n",
    "    \n",
    "    return comparison_metrics\n",
    "\n",
    "def calculate_erd_quality_score(erd_data: Dict) -> Dict[str, float]:\n",
    "    \"\"\"ERD 설계 품질 점수를 계산\"\"\"\n",
    "    quality_metrics = {\n",
    "        'completeness_score': 0.0,    # 완성도 (필수 요소 포함 여부)\n",
    "        'complexity_score': 0.0,      # 복잡도 (적절한 테이블/관계 수)\n",
    "        'normalization_score': 0.0,   # 정규화 점수 (기본키/외래키 적절성)\n",
    "        'design_quality_score': 0.0   # 설계 품질 종합 점수\n",
    "    }\n",
    "    \n",
    "    validation = validate_erd_structure(erd_data)\n",
    "    \n",
    "    # 1. 완성도 점수\n",
    "    completeness_factors = []\n",
    "    completeness_factors.append(100 if validation['has_erd_tables'] else 0)\n",
    "    completeness_factors.append(100 if validation['has_erd_relationships'] else 0)\n",
    "    completeness_factors.append(100 if validation['table_count'] >= 3 else validation['table_count'] * 33.3)\n",
    "    completeness_factors.append(100 if validation['primary_key_count'] >= validation['table_count'] else \n",
    "                               (validation['primary_key_count'] / max(validation['table_count'], 1)) * 100)\n",
    "    \n",
    "    quality_metrics['completeness_score'] = sum(completeness_factors) / len(completeness_factors)\n",
    "    \n",
    "    # 2. 복잡도 점수 (적절한 복잡도인가)\n",
    "    table_count = validation['table_count']\n",
    "    relationship_count = validation['relationship_count']\n",
    "    \n",
    "    # 3-10개 테이블이 적절, 관계는 테이블 수와 비슷하거나 적게\n",
    "    if 3 <= table_count <= 10:\n",
    "        complexity_table_score = 100\n",
    "    elif table_count < 3:\n",
    "        complexity_table_score = table_count * 33.3\n",
    "    else:\n",
    "        complexity_table_score = max(0, 100 - (table_count - 10) * 5)\n",
    "    \n",
    "    if relationship_count <= table_count:\n",
    "        complexity_rel_score = 100\n",
    "    else:\n",
    "        complexity_rel_score = max(0, 100 - (relationship_count - table_count) * 10)\n",
    "    \n",
    "    quality_metrics['complexity_score'] = (complexity_table_score + complexity_rel_score) / 2\n",
    "    \n",
    "    # 3. 정규화 점수\n",
    "    normalization_factors = []\n",
    "    \n",
    "    # 기본키 존재율\n",
    "    if validation['table_count'] > 0:\n",
    "        pk_ratio = validation['primary_key_count'] / validation['table_count']\n",
    "        normalization_factors.append(min(pk_ratio * 100, 100))\n",
    "    \n",
    "    # 외래키 활용도\n",
    "    if validation['relationship_count'] > 0:\n",
    "        fk_ratio = validation['foreign_key_count'] / validation['relationship_count']\n",
    "        normalization_factors.append(min(fk_ratio * 100, 100))\n",
    "    else:\n",
    "        normalization_factors.append(100)  # 관계가 없으면 만점\n",
    "    \n",
    "    if normalization_factors:\n",
    "        quality_metrics['normalization_score'] = sum(normalization_factors) / len(normalization_factors)\n",
    "    \n",
    "    # 4. 설계 품질 종합 점수\n",
    "    quality_metrics['design_quality_score'] = (\n",
    "        quality_metrics['completeness_score'] * 0.4 +\n",
    "        quality_metrics['complexity_score'] * 0.3 +\n",
    "        quality_metrics['normalization_score'] * 0.3\n",
    "    )\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# ========================================================================================\n",
    "# API 호출 및 메인 평가 함수\n",
    "# ========================================================================================\n",
    "\n",
    "def format_data_for_prompt(data):\n",
    "    \"\"\"데이터를 프롬프트에 적합한 형태로 포맷팅\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    elif isinstance(data, (dict, list)):\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        return str(data)\n",
    "\n",
    "def openai_chat_completion(model, user_input, project_info):\n",
    "    \"\"\"OpenAI Chat Completion API를 호출하는 함수\"\"\"\n",
    "    try:\n",
    "        formatted_project_info = format_data_for_prompt(project_info)\n",
    "        formatted_user_input = format_data_for_prompt(user_input)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "프로젝트 정보:\n",
    "{formatted_project_info}\n",
    "\n",
    "요구사항:\n",
    "{formatted_user_input}\n",
    "\n",
    "**필수 JSON 형식:**\n",
    "{{\n",
    "  \"erd_tables\": [{{\n",
    "    \"name\": \"테이블명\",\n",
    "    \"erd_columns\": [{{\n",
    "      \"name\": \"컬럼명\",\n",
    "      \"data_type\": \"타입\",\n",
    "      \"is_primary_key\": true/false,\n",
    "      \"is_foreign_key\": true/false,\n",
    "      \"is_nullable\": true/false\n",
    "    }}]\n",
    "  }}],\n",
    "  \"erd_relationships\": [{{\n",
    "    \"from_table\": \"시작테이블\",\n",
    "    \"to_table\": \"끝테이블\",\n",
    "    \"relationship_type\": \"one-to-many\",\n",
    "    \"foreign_key\": \"외래키명\",\n",
    "    \"constraint_name\": \"제약조건명\"\n",
    "  }}]\n",
    "}}\n",
    "**핵심 규칙:**\n",
    "1. 관계의 모든 테이블명이 erd_tables에 존재해야 함\n",
    "2. 관계의 모든 foreign_key가 해당 테이블 컬럼에 존재해야 함\n",
    "3. 외래키는 is_foreign_key: true 설정\n",
    "4. 최소 5개 테이블, 백슬래시 금지, 순수 JSON만\n",
    "위 규칙을 지켜 완전한 ERD를 생성하세요!\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"API 호출 오류 ({model}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def run_erd_specialized_evaluation():\n",
    "    \"\"\"ERD 특화 3모델 비교 평가 실행\"\"\"\n",
    "    print(\"🎯 ERD 특화 3모델 성능 비교: 구조적 품질과 정확성 중심 평가\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 1. 데이터 로드 및 검증\n",
    "    # ========================================================================================\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_FILE_PATH)\n",
    "        print(f\"✅ 데이터 로드 완료: {len(df)}개 샘플\")\n",
    "        print(f\"📋 CSV 컬럼: {list(df.columns)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CSV 파일 로드 실패: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 데이터 전처리\n",
    "    print(\"\\n🔄 데이터 전처리 중...\")\n",
    "    for col in ['user_input', 'project_info', 'total_requirements', 'ERD_data']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: safe_parse_json(str(x))[0] if pd.notna(x) else {})\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 2. 테스트 샘플 설정 (더 많은 샘플로 확장)\n",
    "    # ========================================================================================\n",
    "    \n",
    "    test_samples = min(5, len(df))  # 20개 샘플로 확장\n",
    "    test_df = df.head(test_samples).copy()\n",
    "    \n",
    "    print(f\"\\n🔄 {test_samples}개 샘플에 대해 3개 모델 ERD 생성 시작...\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 3. 3개 모델 추론 실행\n",
    "    # ========================================================================================\n",
    "    \n",
    "    results = {\n",
    "        'finetuned': {'predictions': [], 'model_name': '파인튜닝_모델'},\n",
    "        'mini': {'predictions': [], 'model_name': 'GPT-4o-mini'},\n",
    "        'gpt4o': {'predictions': [], 'model_name': 'GPT-4o'}\n",
    "    }\n",
    "    \n",
    "    models = [\n",
    "        (FINETUNED_MODEL, 'finetuned', '🎯'),\n",
    "        (BASELINE_MINI_MODEL, 'mini', '🤖'),\n",
    "        (BASELINE_4O_MODEL, 'gpt4o', '🔥')\n",
    "    ]\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"모델 추론 진행\"):\n",
    "        user_input = row['user_input']\n",
    "        project_info = row['project_info']\n",
    "        \n",
    "        print(f\"\\n📝 샘플 {idx+1} 처리 중...\")\n",
    "        \n",
    "        for model_id, result_key, emoji in models:\n",
    "            print(f\"   {emoji} {results[result_key]['model_name']} 추론...\")\n",
    "            \n",
    "            prediction = openai_chat_completion(model_id, user_input, project_info)\n",
    "            results[result_key]['predictions'].append(prediction)\n",
    "            \n",
    "            time.sleep(0.5)  # API 레이트 리미트 방지\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 4. ERD 특화 평가 메트릭 계산\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(\"\\n📊 ERD 특화 평가 결과 분석\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    references = test_df['ERD_data'].tolist()\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for result_key, result_data in results.items():\n",
    "        model_name = result_data['model_name']\n",
    "        predictions = result_data['predictions']\n",
    "        \n",
    "        print(f\"\\n🔍 {model_name} 분석 중...\")\n",
    "        \n",
    "        # JSON 파싱 성공률 계산\n",
    "        parsed_predictions = []\n",
    "        json_success_count = 0\n",
    "        \n",
    "        for pred in predictions:\n",
    "            parsed_pred, is_valid = safe_parse_json(pred)\n",
    "            parsed_predictions.append(parsed_pred)\n",
    "            if is_valid:\n",
    "                json_success_count += 1\n",
    "        \n",
    "        json_success_rate = (json_success_count / len(predictions)) * 100\n",
    "        \n",
    "        # ERD 구조 검증\n",
    "        structure_validations = [validate_erd_structure(pred) for pred in parsed_predictions]\n",
    "        \n",
    "        # 스키마 일관성 계산\n",
    "        consistency_scores = [calculate_schema_consistency(pred) for pred in parsed_predictions]\n",
    "        \n",
    "        # ERD 품질 점수 계산\n",
    "        quality_scores = [calculate_erd_quality_score(pred) for pred in parsed_predictions]\n",
    "        \n",
    "        # 참조 데이터와 비교\n",
    "        comparison_scores = []\n",
    "        for pred, ref in zip(parsed_predictions, references):\n",
    "            if isinstance(ref, dict) and ref:\n",
    "                comparison = compare_erd_structures(pred, ref)\n",
    "                comparison_scores.append(comparison)\n",
    "            else:\n",
    "                comparison_scores.append({\n",
    "                    'table_name_similarity': 0.0,\n",
    "                    'column_structure_similarity': 0.0,\n",
    "                    'relationship_similarity': 0.0,\n",
    "                    'overall_structure_similarity': 0.0\n",
    "                })\n",
    "        \n",
    "        # 평균 계산\n",
    "        avg_metrics = {\n",
    "            'JSON_파싱_성공률(%)': json_success_rate,\n",
    "            '평균_테이블_수': np.mean([v['table_count'] for v in structure_validations]),\n",
    "            '평균_관계_수': np.mean([v['relationship_count'] for v in structure_validations]),\n",
    "            '평균_컬럼_수': np.mean([v['column_count'] for v in structure_validations]),\n",
    "            '외래키_일관성(%)': np.mean([c['foreign_key_consistency'] for c in consistency_scores]),\n",
    "            '관계_일관성(%)': np.mean([c['relationship_consistency'] for c in consistency_scores]),\n",
    "            '네이밍_일관성(%)': np.mean([c['naming_consistency'] for c in consistency_scores]),\n",
    "            '완성도_점수(%)': np.mean([q['completeness_score'] for q in quality_scores]),\n",
    "            '복잡도_점수(%)': np.mean([q['complexity_score'] for q in quality_scores]),\n",
    "            '정규화_점수(%)': np.mean([q['normalization_score'] for q in quality_scores]),\n",
    "            '설계품질_점수(%)': np.mean([q['design_quality_score'] for q in quality_scores]),\n",
    "            '테이블명_일치도(%)': np.mean([c['table_name_similarity'] for c in comparison_scores]),\n",
    "            '컬럼구조_일치도(%)': np.mean([c['column_structure_similarity'] for c in comparison_scores]),\n",
    "            '관계구조_일치도(%)': np.mean([c['relationship_similarity'] for c in comparison_scores]),\n",
    "            '전체구조_일치도(%)': np.mean([c['overall_structure_similarity'] for c in comparison_scores])\n",
    "        }\n",
    "        \n",
    "        evaluation_results[model_name] = avg_metrics\n",
    "        \n",
    "        print(f\"   JSON 파싱 성공률: {json_success_rate:.1f}%\")\n",
    "        print(f\"   평균 테이블 수: {avg_metrics['평균_테이블_수']:.1f}\")\n",
    "        print(f\"   설계 품질 점수: {avg_metrics['설계품질_점수(%)']:.1f}%\")\n",
    "        print(f\"   전체 구조 일치도: {avg_metrics['전체구조_일치도(%)']:.1f}%\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 5. 종합 비교 분석\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n📈 ERD 특화 성능 비교 분석\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # 핵심 메트릭들\n",
    "    key_metrics = [\n",
    "        'JSON_파싱_성공률(%)',\n",
    "        '설계품질_점수(%)',\n",
    "        '전체구조_일치도(%)',\n",
    "        '관계_일관성(%)',\n",
    "        '완성도_점수(%)'\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'메트릭':<20} {'파인튜닝':<12} {'4o-mini':<12} {'GPT-4o':<12} {'vs mini':<12} {'vs 4o':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    ft_results = evaluation_results['파인튜닝_모델']\n",
    "    mini_results = evaluation_results['GPT-4o-mini']\n",
    "    gpt4o_results = evaluation_results['GPT-4o']\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        ft_score = ft_results[metric]\n",
    "        mini_score = mini_results[metric]\n",
    "        gpt4o_score = gpt4o_results[metric]\n",
    "        \n",
    "        vs_mini = ft_score - mini_score\n",
    "        vs_4o = ft_score - gpt4o_score\n",
    "        \n",
    "        print(f\"{metric:<20} {ft_score:<12.1f} {mini_score:<12.1f} {gpt4o_score:<12.1f} {vs_mini:<+12.1f} {vs_4o:<+12.1f}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 6. 세부 분석 결과\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n🔍 세부 ERD 품질 분석\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 구조적 품질 분석\n",
    "    print(\"\\n📊 구조적 품질 분석:\")\n",
    "    structure_metrics = ['평균_테이블_수', '평균_관계_수', '평균_컬럼_수']\n",
    "    \n",
    "    for metric in structure_metrics:\n",
    "        ft_val = ft_results[metric]\n",
    "        mini_val = mini_results[metric]\n",
    "        gpt4o_val = gpt4o_results[metric]\n",
    "        print(f\"  {metric}: 파인튜닝={ft_val:.1f}, 4o-mini={mini_val:.1f}, GPT-4o={gpt4o_val:.1f}\")\n",
    "    \n",
    "    # 일관성 분석\n",
    "    print(\"\\n🔗 스키마 일관성 분석:\")\n",
    "    consistency_metrics = ['외래키_일관성(%)', '관계_일관성(%)', '네이밍_일관성(%)']\n",
    "    \n",
    "    for metric in consistency_metrics:\n",
    "        ft_val = ft_results[metric]\n",
    "        mini_val = mini_results[metric]\n",
    "        gpt4o_val = gpt4o_results[metric]\n",
    "        print(f\"  {metric}: 파인튜닝={ft_val:.1f}%, 4o-mini={mini_val:.1f}%, GPT-4o={gpt4o_val:.1f}%\")\n",
    "    \n",
    "    # 정확성 분석\n",
    "    print(\"\\n🎯 구조 정확성 분석:\")\n",
    "    accuracy_metrics = ['테이블명_일치도(%)', '컬럼구조_일치도(%)', '관계구조_일치도(%)']\n",
    "    \n",
    "    for metric in accuracy_metrics:\n",
    "        ft_val = ft_results[metric]\n",
    "        mini_val = mini_results[metric]\n",
    "        gpt4o_val = gpt4o_results[metric]\n",
    "        print(f\"  {metric}: 파인튜닝={ft_val:.1f}%, 4o-mini={mini_val:.1f}%, GPT-4o={gpt4o_val:.1f}%\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 7. 파인튜닝 효과 분석\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n🎯 파인튜닝 효과 심층 분석\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 베이스 모델 대비 개선도\n",
    "    print(\"📈 베이스 모델(GPT-4o-mini) 대비 파인튜닝 개선도:\")\n",
    "    mini_improvements = 0\n",
    "    total_improvement = 0\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        ft_score = ft_results[metric]\n",
    "        mini_score = mini_results[metric]\n",
    "        diff = ft_score - mini_score\n",
    "        improvement_rate = (diff / mini_score) * 100 if mini_score != 0 else 0\n",
    "        \n",
    "        print(f\"  {metric}: {diff:+.1f} ({improvement_rate:+.1f}%)\")\n",
    "        \n",
    "        if diff > 0:\n",
    "            mini_improvements += 1\n",
    "        total_improvement += improvement_rate\n",
    "    \n",
    "    avg_improvement = total_improvement / len(key_metrics)\n",
    "    \n",
    "    # 플래그십 모델 대비 성능\n",
    "    print(f\"\\n🔥 플래그십 모델(GPT-4o) 대비 파인튜닝 성능:\")\n",
    "    gpt4o_wins = 0\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        ft_score = ft_results[metric]\n",
    "        gpt4o_score = gpt4o_results[metric]\n",
    "        diff = ft_score - gpt4o_score\n",
    "        \n",
    "        status = \"🟢 우수\" if diff > 2 else \"🟡 동등\" if abs(diff) <= 2 else \"🔴 열세\"\n",
    "        print(f\"  {metric}: {diff:+.1f} ({status})\")\n",
    "        \n",
    "        if diff > 0:\n",
    "            gpt4o_wins += 1\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 8. 모델별 강점 분석\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n⚡ 모델별 강점 분석\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 각 모델이 가장 우수한 메트릭 찾기\n",
    "    all_metrics = list(ft_results.keys())\n",
    "    model_strengths = {\n",
    "        '파인튜닝_모델': [],\n",
    "        'GPT-4o-mini': [],\n",
    "        'GPT-4o': []\n",
    "    }\n",
    "    \n",
    "    for metric in all_metrics:\n",
    "        scores = {\n",
    "            '파인튜닝_모델': ft_results[metric],\n",
    "            'GPT-4o-mini': mini_results[metric],\n",
    "            'GPT-4o': gpt4o_results[metric]\n",
    "        }\n",
    "        best_model = max(scores, key=scores.get)\n",
    "        model_strengths[best_model].append((metric, scores[best_model]))\n",
    "    \n",
    "    for model_name, strengths in model_strengths.items():\n",
    "        if strengths:\n",
    "            print(f\"\\n🏆 {model_name} 최우수 영역:\")\n",
    "            for metric, score in strengths[:5]:  # 상위 5개만 표시\n",
    "                print(f\"  • {metric}: {score:.1f}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 9. 실용성 평가\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n💼 실용성 평가\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # JSON 생성 안정성 평가\n",
    "    json_success_rates = {\n",
    "        '파인튜닝': ft_results['JSON_파싱_성공률(%)'],\n",
    "        '4o-mini': mini_results['JSON_파싱_성공률(%)'],\n",
    "        'GPT-4o': gpt4o_results['JSON_파싱_성공률(%)']\n",
    "    }\n",
    "    \n",
    "    print(\"🔧 JSON 생성 안정성:\")\n",
    "    for model, rate in json_success_rates.items():\n",
    "        stability = \"🟢 매우안정\" if rate >= 90 else \"🟡 보통\" if rate >= 70 else \"🔴 불안정\"\n",
    "        print(f\"  {model}: {rate:.1f}% ({stability})\")\n",
    "    \n",
    "    # ERD 설계 품질 종합\n",
    "    design_quality = {\n",
    "        '파인튜닝': ft_results['설계품질_점수(%)'],\n",
    "        '4o-mini': mini_results['설계품질_점수(%)'],\n",
    "        'GPT-4o': gpt4o_results['설계품질_점수(%)']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n🎨 ERD 설계 품질:\")\n",
    "    for model, quality in design_quality.items():\n",
    "        grade = \"🟢 우수\" if quality >= 80 else \"🟡 양호\" if quality >= 60 else \"🔴 개선필요\"\n",
    "        print(f\"  {model}: {quality:.1f}% ({grade})\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 10. 최종 결론 및 권장사항\n",
    "    # ========================================================================================\n",
    "    \n",
    "    print(f\"\\n🏆 최종 결론 및 권장사항\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 파인튜닝 효과 판정\n",
    "    if mini_improvements >= 3 and avg_improvement > 5:\n",
    "        ft_verdict = \"🟢 파인튜닝 대성공! 베이스 모델 대비 명확한 개선\"\n",
    "    elif mini_improvements >= 2:\n",
    "        ft_verdict = \"🟡 파인튜닝 효과 있음. 일부 영역에서 개선\"\n",
    "    else:\n",
    "        ft_verdict = \"🔴 파인튜닝 효과 제한적. 추가 최적화 필요\"\n",
    "    \n",
    "    print(f\"📊 파인튜닝 효과: {ft_verdict}\")\n",
    "    print(f\"   • {mini_improvements}/{len(key_metrics)} 주요 메트릭에서 개선\")\n",
    "    print(f\"   • 평균 개선율: {avg_improvement:+.1f}%\")\n",
    "    \n",
    "    # GPT-4o 대비 성능\n",
    "    if gpt4o_wins >= 3:\n",
    "        gpt4o_verdict = \"🔥 플래그십 모델을 능가하는 놀라운 성과!\"\n",
    "    elif gpt4o_wins >= 2:\n",
    "        gpt4o_verdict = \"⚡ 플래그십 모델과 경쟁 가능한 수준\"\n",
    "    else:\n",
    "        gpt4o_verdict = \"📈 플래그십 모델 대비 개선 여지 존재\"\n",
    "    \n",
    "    print(f\"\\n🎯 GPT-4o 대비: {gpt4o_verdict}\")\n",
    "    print(f\"   • {gpt4o_wins}/{len(key_metrics)} 주요 메트릭에서 우세\")\n",
    "    \n",
    "    # 실용적 권장사항\n",
    "    print(f\"\\n💡 실용적 권장사항:\")\n",
    "    \n",
    "    # 가장 우수한 모델 선택\n",
    "    overall_scores = {\n",
    "        '파인튜닝': np.mean([ft_results[m] for m in key_metrics]),\n",
    "        '4o-mini': np.mean([mini_results[m] for m in key_metrics]),\n",
    "        'GPT-4o': np.mean([gpt4o_results[m] for m in key_metrics])\n",
    "    }\n",
    "    \n",
    "    best_model = max(overall_scores, key=overall_scores.get)\n",
    "    \n",
    "    if best_model == '파인튜닝':\n",
    "        print(\"  🎯 파인튜닝 모델 사용 권장:\")\n",
    "        print(\"    - ERD 특화 작업에 최적화됨\")\n",
    "        print(\"    - 비용 효율적 (GPT-4o 대비 ~90% 절약)\")\n",
    "        print(\"    - 빠른 응답 속도\")\n",
    "    elif best_model == 'GPT-4o':\n",
    "        print(\"  🔥 GPT-4o 사용 권장:\")\n",
    "        print(\"    - 최고 품질의 ERD 설계\")\n",
    "        print(\"    - 복잡한 요구사항 처리 우수\")\n",
    "        print(\"    - 높은 일관성\")\n",
    "    else:\n",
    "        print(\"  🤖 GPT-4o-mini 사용 권장:\")\n",
    "        print(\"    - 균형잡힌 성능\")\n",
    "        print(\"    - 적당한 비용\")\n",
    "        print(\"    - 안정적인 결과\")\n",
    "    \n",
    "    # 비용 대비 성능 분석\n",
    "    print(f\"\\n💰 비용 대비 성능 분석:\")\n",
    "    print(f\"  파인튜닝 모델: 성능 {overall_scores['파인튜닝']:.1f}, 비용 ⭐\")\n",
    "    print(f\"  GPT-4o-mini: 성능 {overall_scores['4o-mini']:.1f}, 비용 ⭐⭐\")\n",
    "    print(f\"  GPT-4o: 성능 {overall_scores['GPT-4o']:.1f}, 비용 ⭐⭐⭐⭐⭐\")\n",
    "    \n",
    "    roi_scores = {\n",
    "        '파인튜닝': overall_scores['파인튜닝'] * 5,  # 비용이 1/5이므로 5배 가중\n",
    "        '4o-mini': overall_scores['4o-mini'] * 2,   # 비용이 1/2이므로 2배 가중\n",
    "        'GPT-4o': overall_scores['GPT-4o']          # 기준\n",
    "    }\n",
    "    \n",
    "    best_roi = max(roi_scores, key=roi_scores.get)\n",
    "    print(f\"  💎 최고 ROI: {best_roi} (비용효율점수: {roi_scores[best_roi]:.1f})\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # 11. 결과 저장\n",
    "    # ========================================================================================\n",
    "    \n",
    "    try:\n",
    "        # 상세 결과 저장\n",
    "        detailed_results = {\n",
    "            'evaluation_summary': {\n",
    "                'test_samples': test_samples,\n",
    "                'evaluation_date': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'key_findings': {\n",
    "                    'finetuning_improvements': mini_improvements,\n",
    "                    'avg_improvement_rate': avg_improvement,\n",
    "                    'gpt4o_competitive_metrics': gpt4o_wins,\n",
    "                    'best_overall_model': best_model,\n",
    "                    'best_roi_model': best_roi\n",
    "                }\n",
    "            },\n",
    "            'detailed_metrics': evaluation_results,\n",
    "            'model_strengths': model_strengths\n",
    "        }\n",
    "        \n",
    "        with open(\"ERD특화_3모델_성능비교_결과.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(detailed_results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\n💾 상세 결과가 'ERD특화_3모델_성능비교_결과.json' 파일로 저장되었습니다.\")\n",
    "        \n",
    "        # 요약 리포트 저장\n",
    "        summary_report = f\"\"\"\n",
    "ERD 특화 3모델 성능 비교 요약 리포트\n",
    "=================================\n",
    "\n",
    "📊 테스트 개요:\n",
    "- 샘플 수: {test_samples}개\n",
    "- 평가 일시: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- 평가 방식: ERD 구조적 품질 중심\n",
    "\n",
    "🏆 주요 결과:\n",
    "- 파인튜닝 효과: {mini_improvements}/{len(key_metrics)} 메트릭 개선 (평균 {avg_improvement:+.1f}%)\n",
    "- GPT-4o 대비: {gpt4o_wins}/{len(key_metrics)} 메트릭 우세\n",
    "- 최고 성능 모델: {best_model}\n",
    "- 최고 ROI 모델: {best_roi}\n",
    "\n",
    "📈 핵심 메트릭 비교:\n",
    "\"\"\"\n",
    "        \n",
    "        for metric in key_metrics:\n",
    "            ft_score = ft_results[metric]\n",
    "            mini_score = mini_results[metric]\n",
    "            gpt4o_score = gpt4o_results[metric]\n",
    "            summary_report += f\"- {metric}: 파인튜닝={ft_score:.1f}, 4o-mini={mini_score:.1f}, GPT-4o={gpt4o_score:.1f}\\n\"\n",
    "        \n",
    "        summary_report += f\"\"\"\n",
    "💡 권장사항:\n",
    "- {ft_verdict}\n",
    "- {gpt4o_verdict}\n",
    "- 비용 효율성: {best_roi} 모델 권장\n",
    "\n",
    "자세한 분석 결과는 'ERD특화_3모델_성능비교_결과.json' 파일을 참조하세요.\n",
    "\"\"\"\n",
    "        \n",
    "        with open(\"ERD특화_성능비교_요약.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summary_report)\n",
    "        \n",
    "        print(f\"📄 요약 리포트가 'ERD특화_성능비교_요약.txt' 파일로 저장되었습니다.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 결과 저장 실패: {e}\")\n",
    "\n",
    "# ========================================================================================\n",
    "# 메인 실행 부분\n",
    "# ========================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"❌ OPENAI_API_KEY 환경변수를 설정해주세요.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"🚀 ERD 특화 3모델 성능 비교를 시작합니다...\")\n",
    "    print(f\"📁 테스트 데이터: {CSV_FILE_PATH}\")\n",
    "    print(f\"🎯 파인튜닝 모델: {FINETUNED_MODEL}\")\n",
    "    print(f\"🤖 GPT-4o-mini: {BASELINE_MINI_MODEL}\")\n",
    "    print(f\"🔥 GPT-4o: {BASELINE_4O_MODEL}\")\n",
    "    print(f\"🌡️ 온도 설정: {TEMPERATURE}\")\n",
    "    print(f\"📊 평가 방식: ERD 구조적 품질 및 정확성 중심\")\n",
    "    \n",
    "    run_erd_specialized_evaluation()\n",
    "    \n",
    "    print(\"\\n✅ ERD 특화 3모델 성능 비교가 완료되었습니다!\")\n",
    "    print(\"📋 결과 파일:\")\n",
    "    print(\"   • ERD특화_3모델_성능비교_결과.json (상세 결과)\")\n",
    "    print(\"   • ERD특화_성능비교_요약.txt (요약 리포트)\")\n",
    "    print(\"\\n🎯 이제 ERD 설계에 최적화된 평가로 정확한 모델 성능을 확인할 수 있습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bab238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
